{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ocr_transformer.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/michaelgfalk/clean-ocr/blob/master/ocr_transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "XQdbN2CEIgt5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Waves of Words: Correcting Trove's Messy OCR\n",
        "\n",
        "One aim of the *Waves of Words* project is to extract Aboriginal wordlists from [Trove](https://trove.nla.gov.au). A challenge we face is that historical newspapers are difficult to OCR, so many of the texts are riddled with errors.\n",
        "\n",
        "Using the training data available from the ALTA 2017 OCR competition, can we create a model that will clean the text enough for our aboriginal word detector to work?\n",
        "\n",
        "I have been giving some thought to whether uppercase letters and punctuation should be preserved in this model, given that the aim is to clean up the text for our detector, which only requires lower case letters and ignores punctuation. I think we need to include all the characters in this one. The extra information about sentence barriers, for example, should hopefully help the model as it would a human when it tries to correct the text. Moreover, many OCR errors involve exchaning punctuation or digits for letters, e.g. `l = 1 = !`.\n",
        "\n",
        "**References:**\n",
        "\n",
        "* D. Mollá, S. Cassidy. Overview of the 2017 ALTA Shared Task:\n",
        "Correcting OCR Errors (2017). *Proc. ALTA 2017*.\n",
        "[https://aclanthology.coli.uni-saarland.de/papers/U17-1014/u17-1014](https://aclanthology.coli.uni-saarland.de/papers/U17-1014/u17-1014)"
      ]
    },
    {
      "metadata": {
        "id": "PzfYL5jcnT7m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Install TensorFlow2\n",
        "!pip install -q tensorflow-gpu==2.0.0-alpha0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ePS0gU47IW-u",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "import csv\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "from datetime import date\n",
        "import pickle as p\n",
        "\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-V1HqBKxgqry",
        "colab_type": "code",
        "outputId": "ebe464a8-d10a-4b48-f7f1-e2ac46f8e0c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "# Mount google drive to get training data. Set data_dir\n",
        "drive.mount('/content/gdrive')\n",
        "data_dir = '/content/gdrive/My Drive/waves_of_words/ocr_correction_data/'"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Yur9PBcBzjBp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 1. Data Pipeline (new)\n",
        "\n",
        "This new data pipeline makes use of Tensorflows Dataset object to manage the extraction and transformation more efficiently."
      ]
    },
    {
      "metadata": {
        "id": "-JwKCURv0I44",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_dataset(source, target, start = 'स', end = 'ए'):\n",
        "  \"\"\"Imports articles from csv and packages for training.\"\"\"\n",
        "  \n",
        "  # Lists for import\n",
        "  raw_x = []\n",
        "  raw_y = []\n",
        "  \n",
        "  # Import raw text\n",
        "  with open(source, \"rt\") as f:\n",
        "    x_reader = csv.reader(f, delimiter = ',', quotechar = '\"')\n",
        "    for row in x_reader:\n",
        "      raw_x.append(row[1])\n",
        "  with open(target, \"rt\") as f:\n",
        "    y_reader = csv.reader(f, delimiter = ',', quotechar = '\"')\n",
        "    for row in y_reader:\n",
        "      raw_y.append(row[1])\n",
        "  \n",
        "  # Drop header rows\n",
        "  raw_x = raw_x[1:]\n",
        "  raw_y = raw_y[1:]\n",
        "  \n",
        "  # Add special characters\n",
        "  x = [start + article + end for article in raw_x if start not in article and end not in article]\n",
        "  y = [start + article + end for article in raw_y if start not in article and end not in article]\n",
        "  \n",
        "  return x, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_KIK6ErA3rf5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def tokenize(source, target, tkzr = None):\n",
        "  \"\"\"Instantiates tokenizer if not passed, fits and applies.\"\"\"\n",
        "  \n",
        "  if tkzr is None:\n",
        "    # Fit tokenizer\n",
        "    tkzr = tf.keras.preprocessing.text.Tokenizer(\n",
        "      num_words = None,\n",
        "      filters = None,\n",
        "      lower = False,\n",
        "      char_level = True\n",
        "    )\n",
        "    tkzr.fit_on_texts(source + target)\n",
        "  \n",
        "  # Apply to texts\n",
        "  x = tkzr.texts_to_sequences(source)\n",
        "  y = tkzr.texts_to_sequences(target)\n",
        "  \n",
        "  return x, y, tkzr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vO17zE6HPV-n",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def split_and_stack(x, y, max_len, batch_size, drop = False):\n",
        "  \"\"\"Takes as input two python lists, and outputs a list of tensor buckets.\n",
        "  \n",
        "  Arguments:\n",
        "  ==========\n",
        "  x (list): the tokenized source strings\n",
        "  y (list): the tokenized target strings\n",
        "  max_len (int): the maximum number of time steps the model will consider\n",
        "  batch_size (int): the batch size for the training examples\n",
        "  drop (bool): keep the final batch, if len(x) is not a multiple of batch_size?\n",
        "  \n",
        "  Returns:\n",
        "  ==========\n",
        "  bucket_list (list): a list of length m, each item of which is a bucket of\n",
        "    similar-length numpy array\n",
        "  \n",
        "  A bucket is a tensor of shape (2, m_prime, max_len). Training examples are first\n",
        "  bucketed into groups of similar length, and seperated into batches of batch_size.\n",
        "  Each batch is then padded out to an integer multiple of max_len, split into chunks\n",
        "  of length max_len and stacked.\n",
        "  \n",
        "  This bucketing, splitting and stacking allows the data to be fed to a stateful RNN.\n",
        "  \n",
        "  Dimensions:\n",
        "  -The first dimension seperates x examples from y\n",
        "  -The second dimension seperates individual training examples.\n",
        "    m_prime = batch_size * ⌈max_len_x_or_y_within_batch / batch_size⌉ \n",
        "  -The third dimension seperates individual time-steps, and is fixed at max_len\"\"\"\n",
        "  \n",
        "  # Set number of training examples (round up if not dropping)\n",
        "  assert len(x) == len(y)\n",
        "  m = len(x)\n",
        "  \n",
        "  # Sort x and y by sequence length\n",
        "  x_y = sorted(zip(x,y), key = lambda tup: max(len(tup[0]), len(tup[1])), reverse = True)\n",
        "  \n",
        "  # Loop through list and create batches\n",
        "  bucket_list = []\n",
        "  steps_per_epoch = 0\n",
        "  for i in range(0, m, batch_size):\n",
        "    \n",
        "    # Slice and unpack the list\n",
        "    bn = batch_size\n",
        "    if len(x_y) < batch_size and drop:\n",
        "      break\n",
        "    elif len(x_y) < batch_size and not drop:\n",
        "      bn = len(x_y)\n",
        "    \n",
        "    bx, by = zip(*[x_y.pop() for _ in range(bn)])\n",
        "    \n",
        "    # Calculate length boundary and m_prime\n",
        "    bl = max(len(bx[0]), len(by[0])) # Get the length of x or y, whichever is greater\n",
        "    b = max_len - (bl % max_len) + bl # Round up to a multiple of max_len\n",
        "    m_prime = int(b / max_len) # Calculate the new number of rows\n",
        "    \n",
        "    steps_per_epoch += m_prime\n",
        "    \n",
        "    # Pad the sequences\n",
        "    x_pad = pad_sequences(bx, maxlen = b, padding = 'post')\n",
        "    y_pad = pad_sequences(by, maxlen = b, padding = 'post')\n",
        "    \n",
        "    # Flip x on the time dimension\n",
        "    x_flipped = np.flip(x_pad, axis = 1)\n",
        "    \n",
        "    # Split and stack\n",
        "    x_out = np.concatenate(np.split(x_pad, m_prime, axis = 1), axis = 0)\n",
        "    y_out = np.concatenate(np.split(y_pad, m_prime, axis = 1), axis = 0)\n",
        "    \n",
        "    # Covert to dataset and append to list\n",
        "    bucket = tf.data.Dataset.from_tensor_slices((x_out, y_out))\n",
        "    bucket = bucket.batch(bn) # NB: bn = batch_size except on the last batch, if drop != True\n",
        "    bucket_list.append(bucket)\n",
        "    \n",
        "  return bucket_list, steps_per_epoch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0xWguIxG5VHm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_dataset(x_path, y_path, max_len = 20, batch_size = 128, drop = False, test_size = .2):\n",
        "  \n",
        "  x, y = create_dataset(x_path, y_path)\n",
        "  \n",
        "  x, y, tkzr = tokenize(x, y)\n",
        "  \n",
        "  seqs = (x_train, x_test, y_train, y_test) = train_test_split(x, y, test_size = test_size)\n",
        "  \n",
        "  train_buckets, train_steps = split_and_stack(x_train, y_train, max_len, batch_size, drop)\n",
        "  \n",
        "  test_buckets, val_steps = split_and_stack(x_test, y_test, max_len, batch_size, drop)\n",
        "  \n",
        "  return train_buckets, test_buckets, tkzr, seqs, train_steps, val_steps"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IsoTxaMM6Rr4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 2. Model Definition\n",
        "\n",
        "Adapted from TensorFlow docs."
      ]
    },
    {
      "metadata": {
        "id": "ReHt8GAqjmZJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class StatefulGRU(tf.keras.layers.GRU):\n",
        "  \"\"\"GRU layer with all the necessaries.\"\"\"\n",
        "  def __init__(self, units):\n",
        "    super(StatefulGRU, self).__init__(\n",
        "        units = units,\n",
        "        # The following parameters must be set this way\n",
        "        # to use CuDNN on GPU\n",
        "        activation='tanh',\n",
        "        recurrent_activation='sigmoid',\n",
        "        recurrent_dropout=0,\n",
        "        unroll=False,\n",
        "        use_bias=True,\n",
        "        reset_after=True,\n",
        "        # The following parameters are necessary for the\n",
        "        # encoder-decoder architecture\n",
        "        return_sequences=True, \n",
        "        return_state=True,\n",
        "        # Stateful must be 'True' in order\n",
        "        # to link the batches in each hyperbatch\n",
        "        stateful=True,\n",
        "        # Just the standard initializer\n",
        "        recurrent_initializer='glorot_uniform'\n",
        "    )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lOKqMawV6VJB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, num_chars, embedding_dim, enc_units, batch_sz):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.enc_units = enc_units\n",
        "    self.embedding = tf.keras.layers.Embedding(num_chars, embedding_dim)\n",
        "    self.first_gru = StatefulGRU(self.enc_units)\n",
        "    self.second_gru = StatefulGRU(self.enc_units)\n",
        "\n",
        "  def call(self, x, hidden):\n",
        "    x = self.embedding(x)\n",
        "    x = self.first_gru(x, initial_state = hidden)\n",
        "    output, state = self.second_gru(x)\n",
        "    return output, state\n",
        "\n",
        "  def initialize_hidden_state(self):\n",
        "    return tf.zeros((self.batch_sz, self.enc_units))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FDgYbbYn7GiP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class BahdanauAttention(tf.keras.Model):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "  \n",
        "  def call(self, query, values):\n",
        "    # hidden shape == (batch_size, hidden size)\n",
        "    # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "    # we are doing this to perform addition to calculate the score\n",
        "    hidden_with_time_axis = tf.expand_dims(query, 1)\n",
        "\n",
        "    # score shape == (batch_size, max_length, hidden_size)\n",
        "    score = self.V(tf.nn.tanh(\n",
        "        self.W1(values) + self.W2(hidden_with_time_axis)))\n",
        "\n",
        "    # attention_weights shape == (batch_size, max_length, 1)\n",
        "    # we get 1 at the last axis because we are applying score to self.V\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "    # context_vector shape after sum == (batch_size, hidden_size)\n",
        "    context_vector = attention_weights * values\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "    \n",
        "    return context_vector, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yPbOQ2H27a4b",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, num_chars, embedding_dim, dec_units, batch_sz):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.dec_units = dec_units\n",
        "    self.embedding = tf.keras.layers.Embedding(num_chars, embedding_dim)\n",
        "    self.first_gru = StatefulGRU(self.dec_units)\n",
        "    self.second_gru = StatefulGRU(self.dec_units)\n",
        "    self.fc = tf.keras.layers.Dense(num_chars)\n",
        "\n",
        "    # used for attention\n",
        "    self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "  def call(self, x, hidden, enc_output):\n",
        "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "\n",
        "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "    # passing the concatenated vector to the GRUs\n",
        "    x = self.first_gru(x)\n",
        "    output, state = self.second_gru(x)\n",
        "\n",
        "    # output shape == (batch_size * 1, hidden_size)\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "    # output shape == (batch_size, vocab)\n",
        "    x = self.fc(output)\n",
        "\n",
        "    return x, state, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-ow7AR1fHKMp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 3. Set up Training"
      ]
    },
    {
      "metadata": {
        "id": "7ta0lQ46HNHb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  # Mask: ignore the model's predictions where the ground truth is padding\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  \n",
        "  # Calculate the loss\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  # Make mask compatible with the loss output\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  \n",
        "  # Multiply the losses by the mask (i.e. zero out all losses where there's just padding)\n",
        "  loss_ *= mask\n",
        "  \n",
        "  return tf.reduce_mean(loss_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_EHWVIilw8GG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Metrics for training\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_acc = tf.keras.metrics.SparseCategoricalAccuracy(name='train_acc')\n",
        "val_loss = tf.keras.metrics.Mean(name='val_loss')\n",
        "val_acc = tf.keras.metrics.SparseCategoricalAccuracy(name='val_acc')\n",
        "\n",
        "def update_accuracy(real, pred, acc_object):\n",
        "  \n",
        "  # Find padding\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  \n",
        "  # If there are no non-padding variables, break out of function\n",
        "  if tf.math.count_nonzero(mask) == 0:\n",
        "    return None\n",
        "  \n",
        "  # Slice tensors\n",
        "  real = tf.boolean_mask(real, mask)\n",
        "  pred = tf.boolean_mask(pred, mask)\n",
        "\n",
        "  # Compute accuracy\n",
        "  acc_object.update_state(real, pred)\n",
        "  \n",
        "  return None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IfTMbgXjHRwP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden, norm_lim):\n",
        "  loss = 0\n",
        "        \n",
        "  with tf.GradientTape() as tape:\n",
        "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "\n",
        "    dec_input = tf.expand_dims(inp[:,0], 1)\n",
        "\n",
        "    # Teacher forcing - feeding the target as the next input\n",
        "    for t in range(1, targ.shape[1]):\n",
        "      # passing enc_output to the decoder\n",
        "      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "\n",
        "      loss += loss_function(targ[:, t], predictions)\n",
        "      _ = update_accuracy(targ[:, t], predictions, train_acc)\n",
        "\n",
        "      # using teacher forcing\n",
        "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "  train_loss.update_state(loss)\n",
        "\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "  \n",
        "  # Clip gradients\n",
        "  clipped_gradients = [tf.clip_by_norm(grad, norm_lim) for grad in gradients]\n",
        "\n",
        "  optimizer.apply_gradients(zip(clipped_gradients, variables))\n",
        "  \n",
        "  return train_loss.result(), train_acc.result()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CCSzwR095CVx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def val_step(inp, targ, enc_hidden):\n",
        "  \n",
        "  loss = 0\n",
        "  \n",
        "  # Begin feeding data to network\n",
        "  enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "  dec_hidden = enc_hidden\n",
        "  dec_input = tf.expand_dims(inp[:,0], 1)\n",
        "  \n",
        "  # Cycle through the rest of the time steps\n",
        "  for t in range(1, targ.shape[1]):\n",
        "    # Pass enc_output to the decoder\n",
        "    predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "    \n",
        "    # Calculate loss and acc\n",
        "    loss += loss_function(targ[:,t], predictions)\n",
        "    _ = update_accuracy(targ[:, t], predictions, val_acc)\n",
        "    \n",
        "    # Pass the next correct letter to the decoder (teacher forcing)\n",
        "    dec_input = tf.expand_dims(targ[:,t], 1)\n",
        "    \n",
        "  # Calculate val_loss\n",
        "  val_loss.update_state(loss)\n",
        "  \n",
        "  return val_loss.result(), val_acc.result()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UwilCQ2uVTSO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def format_time(flt):\n",
        "  h = flt//3600\n",
        "  m = (flt % 3600)//60\n",
        "  s = flt % 60\n",
        "  out = []\n",
        "  if h > 0:\n",
        "    out.append(str(int(h)))\n",
        "    if h == 1:\n",
        "      out.append('hr,')\n",
        "    else:\n",
        "      out.append('hrs,')\n",
        "  if m > 0:\n",
        "    out.append(str(int(m)))\n",
        "    if m == 1:\n",
        "      out.append('min, and')\n",
        "    else:\n",
        "      out.append('mins, and')\n",
        "  out.append(f'{s:.2f}')\n",
        "  out.append('secs')\n",
        "  return ' '.join(out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XPoK-tAHJqaR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 4. Run Training Loop"
      ]
    },
    {
      "metadata": {
        "id": "kRB1G3CCG6Lt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Set hyperparameters\n",
        "MAX_LEN = 32\n",
        "BATCH_SIZE = 128\n",
        "EPOCHS = 2\n",
        "x_dir = data_dir + 'train_input.csv'\n",
        "y_dir = data_dir + 'train_output.csv'\n",
        "NORM_LIM = 3 # value for clip_norm\n",
        "\n",
        "# Load data\n",
        "train_buckets, test_buckets, tkzr, seqs, train_steps, val_steps = load_dataset(x_dir, y_dir, MAX_LEN, BATCH_SIZE, drop = True)\n",
        "\n",
        "# Save preprocessed training data\n",
        "with open(data_dir + \"/\" + date.isoformat(date.today()) + \"-training-data-and-tkzr.pickle\", 'wb') as f:\n",
        "  p.dump((tkzr, seqs, train_steps, val_steps), f)\n",
        "\n",
        "# Get vocab size\n",
        "num_chars = len(tkzr.word_index) + 1 # Add one for padding\n",
        "embedding_dim = 25\n",
        "units = 50\n",
        "\n",
        "# Define model(s)\n",
        "encoder = Encoder(num_chars, embedding_dim, units, batch_sz = BATCH_SIZE)\n",
        "decoder = Decoder(num_chars, embedding_dim, units, batch_sz = BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "E1QIHlHOHOL5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "checkpoint_dir = data_dir + 'checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WgYulu90Igu7",
        "colab_type": "code",
        "outputId": "80d1c709-dfe0-40ef-d188-057458471948",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "cell_type": "code",
      "source": [
        "# Loop over epochs\n",
        "for epoch in range(10):\n",
        "  print(f'Starting Epoch {epoch + 1}\\n')\n",
        "  \n",
        "  train_loss.reset_states()\n",
        "  train_acc.reset_states()\n",
        "  val_loss.reset_states()\n",
        "  val_acc.reset_states()\n",
        "  \n",
        "  start = time.time()\n",
        "  \n",
        "  total_batches = 0\n",
        "  val_batches = 0\n",
        "\n",
        "  # Loop over buckets\n",
        "  for bucket, dataset in enumerate(train_buckets):\n",
        "    # Reset hidden state\n",
        "    enc_hidden = encoder.initialize_hidden_state()\n",
        "\n",
        "\n",
        "    for inp, targ in dataset.take(-1):\n",
        "      loss, acc = train_step(inp, targ, enc_hidden, NORM_LIM)\n",
        "      \n",
        "      total_batches += 1\n",
        "    \n",
        "      if total_batches % 250 == 0:\n",
        "          print(f'Epoch {epoch + 1} Bucket {bucket + 1}: Loss {loss:.4f}, Acc {acc:.4f} after {total_batches} batches')\n",
        "  \n",
        "  # saving (checkpoint) the model every 2 epochs\n",
        "  if (epoch + 1) % 2 == 0:\n",
        "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "    \n",
        "  # Calculate validation loss and accuracy\n",
        "  for dataset in test_buckets:\n",
        "    # Reset hidden state\n",
        "    enc_hidden = encoder.initialize_hidden_state()\n",
        "    \n",
        "    for inp, targ in dataset.take(-1):\n",
        "      val_loss, val_acc = val_step(inp, targ, enc_hidden)\n",
        "      \n",
        "      val_batches += 1\n",
        "\n",
        "  print(f'\\nEpoch {epoch + 1} Loss {loss:.2f}, Avg Acc {acc*100:.2f}%.')\n",
        "  print(f'Tested on {val_batches * BATCH_SIZE} validation examples.')\n",
        "  print(f' val_loss = {val_loss:.2f} val_acc = {val_acc*100:.2f}%')\n",
        "  print(f'Time taken for 1 epoch: {format_time(time.time() - start)}\\n===========================\\n\\n')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting Epoch 1\n",
            "\n",
            "Epoch 1 Bucket 15: Loss 68.4871, Acc 0.3904 after 250 batches\n",
            "Epoch 1 Bucket 21: Loss 67.1030, Acc 0.4041 after 500 batches\n",
            "Epoch 1 Bucket 25: Loss 65.7859, Acc 0.4167 after 750 batches\n",
            "Epoch 1 Bucket 28: Loss 64.5594, Acc 0.4282 after 1000 batches\n",
            "Epoch 1 Bucket 31: Loss 63.4161, Acc 0.4390 after 1250 batches\n",
            "Epoch 1 Bucket 32: Loss 62.4643, Acc 0.4480 after 1500 batches\n",
            "Epoch 1 Bucket 34: Loss 61.6920, Acc 0.4554 after 1750 batches\n",
            "Epoch 1 Bucket 35: Loss 60.9797, Acc 0.4622 after 2000 batches\n",
            "Epoch 1 Bucket 36: Loss 60.4408, Acc 0.4673 after 2250 batches\n",
            "Epoch 1 Bucket 37: Loss 59.9692, Acc 0.4719 after 2500 batches\n",
            "\n",
            "Epoch 1 Loss 59.4617, Avg Acc 0.4767.\n",
            "Tested on 63232 validation examples. val_loss = 52.00727844238281 val_acc = 0.552980363368988\n",
            "Time taken for 1 epoch: 19 mins, and 59.83 secs\n",
            "===========================\n",
            "\n",
            "\n",
            "Starting Epoch 2\n",
            "\n",
            "Epoch 2 Bucket 15: Loss 58.3669, Acc 0.4867 after 250 batches\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "z2xFuk5wRuo-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# See How it Goes"
      ]
    },
    {
      "metadata": {
        "id": "WA9TC2_ZsFA3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "91adecfb-00c8-437b-c31b-34cd82b60f05"
      },
      "cell_type": "code",
      "source": [
        "# Load model if revisiting notebook\n",
        "checkpoint.restore(os.path.join(checkpoint_dir, \"ckpt-3.index\"))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0417 23:56:32.585687 140095537694592 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/util.py:1074: NameBasedSaverStatus.__init__ (from tensorflow.python.training.tracking.util) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Restoring a name-based tf.train.Saver checkpoint using the object-based restore API. This mode uses global names to match variables, and so is somewhat fragile. It also adds new restore ops to the graph each time it is called when graph building. Prefer re-encoding training checkpoints in the object-based format: run save() on the object-based saver (the same one this message is coming from) and use that checkpoint in the future.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.NameBasedSaverStatus at 0x7f6a137210b8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "metadata": {
        "id": "XPqhXHE7Ry_k",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def clean_ocr(article, tkzr, start_char = 'स', end_char = 'ए', max_len = 32):\n",
        "\n",
        "    # Tokenise the article\n",
        "    inputs = [tkzr.word_index[i] for i in list(article)]\n",
        "    \n",
        "    # Pad, split, stack\n",
        "    t = len(inputs)\n",
        "    b = max_len - (t % max_len) + t\n",
        "    m_prime = b // max_len\n",
        "    inputs = pad_sequences(inputs, maxlen = b, padding = 'post')\n",
        "    inputs = np.flip(inputs, axis = 1)\n",
        "    inputs = np.concatenate(np.split(x_pad, m_prime, axis = 1), axis = 0)\n",
        "    inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "    # Create empty string for result\n",
        "    result = ''\n",
        "    \n",
        "    # Set threshold to stop outputting results if 'end' character not reached\n",
        "    give_up = int(len(article) * 1.2)\n",
        "    \n",
        "    # Create empty string for result\n",
        "    result = ''\n",
        "    \n",
        "    # Encode and decode the article\n",
        "    enc_hidden = tf.zeros((1, units))\n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = \n",
        "    for r in range():\n",
        "      # Encode this chunk:\n",
        "      enc_out, enc_hidden = encoder(inputs[r,:], enc_hidden)\n",
        "      \n",
        "\n",
        "    # Ready the decoder\n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = tf.expand_dims([tkzr.word_index[start_char]], 0)\n",
        "\n",
        "    # Start outputting characters till the end character is reached, or\n",
        "    # the output sequence is getting much longer than the input\n",
        "    while len(result) < give_up:\n",
        "      # Get next result from decoder:\n",
        "      predictions, dec_hidden, _ = decoder(dec_input,\n",
        "                                           dec_hidden,\n",
        "                                           enc_out)\n",
        "      \n",
        "      # Which is the next letter?\n",
        "      predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "      \n",
        "      # Add to result\n",
        "      result += tkzr.index_word[predicted_id]\n",
        "\n",
        "      # If we've reached the end, stop\n",
        "      if tkzr.index_word[predicted_id] == end_char:\n",
        "          break\n",
        "\n",
        "      # Otherwise the predicted ID is fed back into the model\n",
        "      dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    return result, article"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sPfWtXJBvFBq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "48508030-ed10-4f73-feb8-aa89188d539e"
      },
      "cell_type": "code",
      "source": [
        "# Let's have a look at a shorter article...\n",
        "random_article = \"BANANA SHIRE COUNCIL MEETING ELECTRICITY SUPPLY FOR BILOELA Request For Investigation Those present at the December meet- ing of the Banana Shire Council were : Mr J. C. Graham (Chairman), E. Brad : shaw, H. R. Brake, C. McDoualV, A. J. McPherson, R. G. Maclean, J. M. Car bery, W. H. Leigh, S. A. Barre, and E. Schucucmann. Leave of absence was granted to Crs Homer and Hamilton. The Commercial Bank (Wowan) ad- vised that the council had Wen granted an overdraft of £3000. The matter of Bathurst burr at Cra cGw, as complained of by the Cracow A.L.P., was left for Cr Bradshaw to investigate. The overseer was instructed to report on the matter of the road through Messrs Heywood's property, and Mr Faulkener to be advised accordingly. The Wowan L.P.A. asked that atten- tion be given to the road from Deeford to the Wowan cemetery, and it was de- cided to attend to it. The Land Commissioner, Rockhamp- ton, advised that the Chief Protector oi Aborigines had applied tor Res. R87 Wright as a reserve for aborigines. It was decided to advise toe council are not in favour of this. The Kokotunga L.P.A. wrote re the state of the Kokotunga-Baralaba read, and thc proposed dam at Kokotunga. It was decided to advise that at present there are no funds available tor the road between Baralaba and Kokotunga, and the matter of the dam is under con- sideration. The overseer wai instructed to inspect the work carried out on the road at Kalewa by D. Halberstater. Messrs E. P. Carige and H. Whlt sides asked that their roads ba repaired, and lt was decided to reply that there ar« no funds available at present for the work, but there may be some loan money available at a latir date. F. R. Wafer drew attention to th« state of the road and crossinjrs from Mrs Mulallyt turn-off to his gat«. It was resolved to advise there are no funds available for the work. The Callide Valley Dairymen's Asso- ciation stated a motion waa passed in connorton with th« necessity of an elec- tricity supply being made available to Biloela and district, and asking for the council's support. It was resolved that Mr T. A. Foley and th« Electricity Commission be written to and asked i' an officer could be sent up to investigate the cost of an electricity supply for Biloela. Mr 8. Epinoff was granted permission to erect gates and grids across th* road at the south-western corner of portion 120, parish qf Grevilla. The Biloela Chamber of Commerce wrote on the cam« subject and it was decided to advise the matter was re- ceiving attention. The overseer will Inspect and report on the road complained of by E. A. Rennets, The Lawgi and Kariboe L.P.A. asked that cream rem te« and the road ' from Lawgi to Fisher's Corner^ne re- paired. It was resolved the work be at- tended to early in the Kew Year. The Biloela Chamber of Commerce asked that Ore ville* Street be gravelled, that danger signs and sign , boards be erected end that Melton Street, Broom tit Street, Bell Street, and the northern end of Kariboe Street be formed and graded, and Rhodes grass cleared.--It was decided to advise that finances do not permit of the streets being graded, : but the grass will be attended to, also other matters complained of. MISCELLANEOUS ITEMS. It was resolved to advise H. D. Hewitt that the council have no objec- tion to his leasing the R21 Gibber- gunyah, provided the right« ot the travelling publio are protected. Application will be made to the I Main Roads Commission to nave the third section ot the Theodore-Cracow Road cleared and formed. H. B. Rickard will be advised that ; there is no objection to hts leasing R62, ptovided he keeps lt free td noxious . weeds and the right« of the travelling ; public are protected. s CHAIRMAN'S VISIT TO DAWXS. The Chairman reported on his vlsi» to Dawes when he attended the meeting of the Lawgi and Dawes Ratepayers' Association. After hearing all their com- plaints re roads, etc., the Chairman said he considered they had a great deal to ' complain of, as practically very little work had been carried with not getting : fair representation, it appeared to the Chairman that possibly some ratepayers would be better served by joining np with the' Monto Shire as there wa« an 1 all-weather road to th» railhead. He also suggested that the overseer, an; available councillors of Division i, »nd . himself make an inspection of the work which requires attending to, eo it can be attended to early In tte new year. Tho clerk was instructed to thank th« members of the Dawes Ratepayers' Asso- ciation for their courtesy to the Chair man on has recent visit, and state ai endeavour will be made to have wort curried out in the new year.\"\n",
        "result, _ = clean_ocr(random_article, tkzr)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([   1 4658], shape=(2,), dtype=int32)\n",
            "tf.Tensor([ 1 30], shape=(2,), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}