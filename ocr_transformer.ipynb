{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ocr_transformer.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/michaelgfalk/clean-ocr/blob/master/ocr_transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "XQdbN2CEIgt5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Waves of Words: Correcting Trove's Messy OCR\n",
        "\n",
        "One aim of the *Waves of Words* project is to extract Aboriginal wordlists from [Trove](https://trove.nla.gov.au). A challenge we face is that historical newspapers are difficult to OCR, so many of the texts are riddled with errors.\n",
        "\n",
        "Using the training data available from the ALTA 2017 OCR competition, can we create a model that will clean the text enough for our aboriginal word detector to work?\n",
        "\n",
        "I have been giving some thought to whether uppercase letters and punctuation should be preserved in this model, given that the aim is to clean up the text for our detector, which only requires lower case letters and ignores punctuation. I think we need to include all the characters in this one. The extra information about sentence barriers, for example, should hopefully help the model as it would a human when it tries to correct the text. Moreover, many OCR errors involve exchaning punctuation or digits for letters, e.g. `l = 1 = !`.\n",
        "\n",
        "**References:**\n",
        "\n",
        "* D. Mollá, S. Cassidy. Overview of the 2017 ALTA Shared Task:\n",
        "Correcting OCR Errors (2017). *Proc. ALTA 2017*.\n",
        "[https://aclanthology.coli.uni-saarland.de/papers/U17-1014/u17-1014](https://aclanthology.coli.uni-saarland.de/papers/U17-1014/u17-1014)"
      ]
    },
    {
      "metadata": {
        "id": "PzfYL5jcnT7m",
        "colab_type": "code",
        "outputId": "382ac448-4945-4917-a5d7-22cc73df4d26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "# Install TensorFlow2\n",
        "!pip install -q tensorflow-gpu==2.0.0-alpha0"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K    100% |████████████████████████████████| 332.1MB 31kB/s \n",
            "\u001b[K    100% |████████████████████████████████| 61kB 12.7MB/s \n",
            "\u001b[K    100% |████████████████████████████████| 3.0MB 2.7MB/s \n",
            "\u001b[K    100% |████████████████████████████████| 419kB 7.7MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ePS0gU47IW-u",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "import csv\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "from datetime import date\n",
        "import pickle as p\n",
        "\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-V1HqBKxgqry",
        "colab_type": "code",
        "outputId": "243aa76b-a3db-4eb6-8ce0-a17da51e68fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "cell_type": "code",
      "source": [
        "# Mount google drive to get training data. Set data_dir\n",
        "drive.mount('/content/gdrive')\n",
        "data_dir = '/content/gdrive/My Drive/waves_of_words/ocr_correction_data/'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Yur9PBcBzjBp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 1. Data Pipeline (new)\n",
        "\n",
        "This new data pipeline makes use of Tensorflows Dataset object to manage the extraction and transformation more efficiently."
      ]
    },
    {
      "metadata": {
        "id": "-JwKCURv0I44",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_dataset(source, target, start = 'स', end = 'ए'):\n",
        "  \"\"\"Imports articles from csv and packages for training.\"\"\"\n",
        "  \n",
        "  # Lists for import\n",
        "  raw_x = []\n",
        "  raw_y = []\n",
        "  \n",
        "  # Import raw text\n",
        "  with open(source, \"rt\") as f:\n",
        "    x_reader = csv.reader(f, delimiter = ',', quotechar = '\"')\n",
        "    for row in x_reader:\n",
        "      raw_x.append(row[1])\n",
        "  with open(target, \"rt\") as f:\n",
        "    y_reader = csv.reader(f, delimiter = ',', quotechar = '\"')\n",
        "    for row in y_reader:\n",
        "      raw_y.append(row[1])\n",
        "  \n",
        "  # Drop header rows\n",
        "  raw_x = raw_x[1:]\n",
        "  raw_y = raw_y[1:]\n",
        "  \n",
        "  # Add special characters\n",
        "  x = [start + article + end for article in raw_x if start not in article and end not in article]\n",
        "  y = [start + article + end for article in raw_y if start not in article and end not in article]\n",
        "  \n",
        "  return x, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_KIK6ErA3rf5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def tokenize(source, target, tkzr = None):\n",
        "  \"\"\"Instantiates tokenizer if not passed, fits and applies.\"\"\"\n",
        "  \n",
        "  if tkzr is None:\n",
        "    # Fit tokenizer\n",
        "    tkzr = tf.keras.preprocessing.text.Tokenizer(\n",
        "      num_words = None,\n",
        "      filters = None,\n",
        "      lower = False,\n",
        "      char_level = True\n",
        "    )\n",
        "    tkzr.fit_on_texts(source + target)\n",
        "  \n",
        "  # Apply to texts\n",
        "  x = tkzr.texts_to_sequences(source)\n",
        "  y = tkzr.texts_to_sequences(target)\n",
        "  \n",
        "  return x, y, tkzr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vO17zE6HPV-n",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def split_and_stack(x, y, max_len, batch_size, drop = False):\n",
        "  \"\"\"Takes as input two python lists, and outputs a list of tensor buckets.\n",
        "  \n",
        "  Arguments:\n",
        "  ==========\n",
        "  x (list): the tokenized source strings\n",
        "  y (list): the tokenized target strings\n",
        "  max_len (int): the maximum number of time steps the model will consider\n",
        "  batch_size (int): the batch size for the training examples\n",
        "  drop (bool): keep the final batch, if len(x) is not a multiple of batch_size?\n",
        "  \n",
        "  Returns:\n",
        "  ==========\n",
        "  bucket_list (list): a list of length m, each item of which is a bucket of\n",
        "    similar-length numpy array\n",
        "  \n",
        "  A bucket is a tensor of shape (2, m_prime, max_len). Training examples are first\n",
        "  bucketed into groups of similar length, and seperated into batches of batch_size.\n",
        "  Each batch is then padded out to an integer multiple of max_len, split into chunks\n",
        "  of length max_len and stacked.\n",
        "  \n",
        "  This bucketing, splitting and stacking allows the data to be fed to a stateful RNN.\n",
        "  \n",
        "  Dimensions:\n",
        "  -The first dimension seperates x examples from y\n",
        "  -The second dimension seperates individual training examples.\n",
        "    m_prime = batch_size * ⌈max_len_x_or_y_within_batch / batch_size⌉ \n",
        "  -The third dimension seperates individual time-steps, and is fixed at max_len\"\"\"\n",
        "  \n",
        "  # Set number of training examples (round up if not dropping)\n",
        "  assert len(x) == len(y)\n",
        "  m = len(x)\n",
        "  \n",
        "  # Sort x and y by sequence length\n",
        "  x_y = sorted(zip(x,y), key = lambda tup: max(len(tup[0]), len(tup[1])), reverse = True)\n",
        "  \n",
        "  # Loop through list and create batches\n",
        "  bucket_list = []\n",
        "  steps_per_epoch = 0\n",
        "  for i in range(0, m, batch_size):\n",
        "    \n",
        "    # Slice and unpack the list\n",
        "    bn = batch_size\n",
        "    if len(x_y) < batch_size and drop:\n",
        "      break\n",
        "    elif len(x_y) < batch_size and not drop:\n",
        "      bn = len(x_y)\n",
        "    \n",
        "    bx, by = zip(*[x_y.pop() for _ in range(bn)])\n",
        "    \n",
        "    # Calculate length boundary and m_prime\n",
        "    bl = max(len(bx[0]), len(by[0])) # Get the length of x or y, whichever is greater\n",
        "    b = max_len - (bl % max_len) + bl # Round up to a multiple of max_len\n",
        "    m_prime = int(b / max_len) # Calculate the new number of rows\n",
        "    \n",
        "    steps_per_epoch += m_prime\n",
        "    \n",
        "    # Pad the sequences\n",
        "    x_pad = pad_sequences(bx, maxlen = b, padding = 'post')\n",
        "    y_pad = pad_sequences(by, maxlen = b, padding = 'post')\n",
        "    \n",
        "    # Flip x on the time dimension\n",
        "    x_flipped = np.flip(x_pad, axis = 1)\n",
        "    \n",
        "    # Split and stack\n",
        "    x_out = np.concatenate(np.split(x_pad, m_prime, axis = 1), axis = 0)\n",
        "    y_out = np.concatenate(np.split(y_pad, m_prime, axis = 1), axis = 0)\n",
        "    \n",
        "    # Covert to dataset and append to list\n",
        "    bucket = tf.data.Dataset.from_tensor_slices((x_out, y_out))\n",
        "    bucket = bucket.batch(bn) # NB: bn = batch_size except on the last batch, if drop != True\n",
        "    bucket_list.append(bucket)\n",
        "    \n",
        "  return bucket_list, steps_per_epoch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0xWguIxG5VHm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_dataset(x_path, y_path, max_len = 20, batch_size = 128, drop = False, test_size = .2):\n",
        "  \n",
        "  x, y = create_dataset(x_path, y_path)\n",
        "  \n",
        "  x, y, tkzr = tokenize(x, y)\n",
        "  \n",
        "  seqs = (x_train, x_test, y_train, y_test) = train_test_split(x, y, test_size = test_size)\n",
        "  \n",
        "  train_buckets, train_steps = split_and_stack(x_train, y_train, max_len, batch_size, drop)\n",
        "  \n",
        "  test_buckets, val_steps = split_and_stack(x_test, y_test, max_len, batch_size, drop)\n",
        "  \n",
        "  return train_buckets, test_buckets, tkzr, seqs, train_steps, val_steps"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IsoTxaMM6Rr4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 2. Model Definition\n",
        "\n",
        "Adapted from TensorFlow docs."
      ]
    },
    {
      "metadata": {
        "id": "ReHt8GAqjmZJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class StatefulGRU(tf.keras.layers.GRU):\n",
        "  \"\"\"GRU layer with all the necessaries.\"\"\"\n",
        "  def __init__(self, units):\n",
        "    super(StatefulGRU, self).__init__(\n",
        "        units = units,\n",
        "        # The following parameters must be set this way\n",
        "        # to use CuDNN on GPU\n",
        "        activation='tanh',\n",
        "        recurrent_activation='sigmoid',\n",
        "        recurrent_dropout=0,\n",
        "        unroll=False,\n",
        "        use_bias=True,\n",
        "        reset_after=True,\n",
        "        # The following parameters are necessary for the\n",
        "        # encoder-decoder architecture\n",
        "        return_sequences=True, \n",
        "        return_state=True,\n",
        "        # Stateful must be 'True' in order\n",
        "        # to link the batches in each hyperbatch\n",
        "        stateful=True,\n",
        "        # Just the standard initializer\n",
        "        recurrent_initializer='glorot_uniform'\n",
        "    )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lOKqMawV6VJB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, num_chars, embedding_dim, enc_units, batch_sz):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.enc_units = enc_units\n",
        "    self.embedding = tf.keras.layers.Embedding(num_chars, embedding_dim)\n",
        "    self.first_gru = StatefulGRU(self.enc_units)\n",
        "    self.second_gru = StatefulGRU(self.enc_units)\n",
        "\n",
        "  def call(self, x, hidden):\n",
        "    x = self.embedding(x)\n",
        "    x = self.first_gru(x, initial_state = hidden)\n",
        "    output, state = self.second_gru(x)\n",
        "    return output, state\n",
        "\n",
        "  def initialize_hidden_state(self):\n",
        "    return tf.zeros((self.batch_sz, self.enc_units))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FDgYbbYn7GiP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class BahdanauAttention(tf.keras.Model):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "  \n",
        "  def call(self, query, values):\n",
        "    # hidden shape == (batch_size, hidden size)\n",
        "    # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "    # we are doing this to perform addition to calculate the score\n",
        "    hidden_with_time_axis = tf.expand_dims(query, 1)\n",
        "\n",
        "    # score shape == (batch_size, max_length, hidden_size)\n",
        "    score = self.V(tf.nn.tanh(\n",
        "        self.W1(values) + self.W2(hidden_with_time_axis)))\n",
        "\n",
        "    # attention_weights shape == (batch_size, max_length, 1)\n",
        "    # we get 1 at the last axis because we are applying score to self.V\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "    # context_vector shape after sum == (batch_size, hidden_size)\n",
        "    context_vector = attention_weights * values\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "    \n",
        "    return context_vector, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yPbOQ2H27a4b",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, num_chars, embedding_dim, dec_units, batch_sz):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.dec_units = dec_units\n",
        "    self.embedding = tf.keras.layers.Embedding(num_chars, embedding_dim)\n",
        "    self.first_gru = StatefulGRU(self.dec_units)\n",
        "    self.second_gru = StatefulGRU(self.dec_units)\n",
        "    self.fc = tf.keras.layers.Dense(num_chars)\n",
        "\n",
        "    # used for attention\n",
        "    self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "  def call(self, x, hidden, enc_output):\n",
        "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "\n",
        "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "    # passing the concatenated vector to the GRUs\n",
        "    x = self.first_gru(x)\n",
        "    output, state = self.second_gru(x)\n",
        "\n",
        "    # output shape == (batch_size * 1, hidden_size)\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "    # output shape == (batch_size, vocab)\n",
        "    x = self.fc(output)\n",
        "\n",
        "    return x, state, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-ow7AR1fHKMp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 3. Set up Training"
      ]
    },
    {
      "metadata": {
        "id": "7ta0lQ46HNHb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  # Mask: ignore the model's predictions where the ground truth is padding\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  \n",
        "  # Calculate the loss\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  # Make mask compatible with the loss output\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  \n",
        "  # Multiply the losses by the mask (i.e. zero out all losses where there's just padding)\n",
        "  loss_ *= mask\n",
        "  \n",
        "  return tf.reduce_mean(loss_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_EHWVIilw8GG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Metrics for training\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_acc = tf.keras.metrics.SparseCategoricalAccuracy(name='train_acc')\n",
        "val_loss = tf.keras.metrics.Mean(name='val_loss')\n",
        "val_acc = tf.keras.metrics.SparseCategoricalAccuracy(name='val_acc')\n",
        "\n",
        "def update_accuracy(real, pred, acc_object):\n",
        "  \n",
        "  # Find padding\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  \n",
        "  # If there are no non-padding variables, break out of function\n",
        "  if tf.math.count_nonzero(mask) == 0:\n",
        "    return None\n",
        "  \n",
        "  # Slice tensors\n",
        "  real = tf.boolean_mask(real, mask)\n",
        "  pred = tf.boolean_mask(pred, mask)\n",
        "\n",
        "  # Compute accuracy\n",
        "  acc_object.update_state(real, pred)\n",
        "  \n",
        "  return None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IfTMbgXjHRwP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden, norm_lim):\n",
        "  loss = 0\n",
        "        \n",
        "  with tf.GradientTape() as tape:\n",
        "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "\n",
        "    dec_input = tf.expand_dims(inp[:,0], 1)\n",
        "\n",
        "    # Teacher forcing - feeding the target as the next input\n",
        "    for t in range(1, targ.shape[1]):\n",
        "      # passing enc_output to the decoder\n",
        "      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "\n",
        "      loss += loss_function(targ[:, t], predictions)\n",
        "      _ = update_accuracy(targ[:, t], predictions, train_acc)\n",
        "\n",
        "      # using teacher forcing\n",
        "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "  train_loss.update_state(loss)\n",
        "\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "  \n",
        "  # Clip gradients\n",
        "  clipped_gradients = [tf.clip_by_norm(grad, norm_lim) for grad in gradients]\n",
        "\n",
        "  optimizer.apply_gradients(zip(clipped_gradients, variables))\n",
        "  \n",
        "  return train_loss.result(), train_acc.result()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CCSzwR095CVx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def val_step(inp, targ, enc_hidden):\n",
        "  \n",
        "  loss = 0\n",
        "  \n",
        "  # Begin feeding data to network\n",
        "  enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "  dec_hidden = enc_hidden\n",
        "  dec_input = tf.expand_dims(inp[:,0], 1)\n",
        "  \n",
        "  # Cycle through the rest of the time steps\n",
        "  for t in range(1, targ.shape[1]):\n",
        "    # Pass enc_output to the decoder\n",
        "    predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "    \n",
        "    # Calculate loss and acc\n",
        "    loss += loss_function(targ[:,t], predictions)\n",
        "    _ = update_accuracy(targ[:, t], predictions, val_acc)\n",
        "    \n",
        "    # Pass the next correct letter to the decoder (teacher forcing)\n",
        "    dec_input = tf.expand_dims(targ[:,t], 1)\n",
        "    \n",
        "  # Calculate val_loss\n",
        "  val_loss.update_state(loss)\n",
        "  \n",
        "  return val_loss.result(), val_acc.result()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UwilCQ2uVTSO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def format_time(flt):\n",
        "  h = flt//3600\n",
        "  m = (flt % 3600)//60\n",
        "  s = flt % 60\n",
        "  out = []\n",
        "  if h > 0:\n",
        "    out.append(str(int(h)))\n",
        "    if h == 1:\n",
        "      out.append('hr,')\n",
        "    else:\n",
        "      out.append('hrs,')\n",
        "  if m > 0:\n",
        "    out.append(str(int(m)))\n",
        "    if m == 1:\n",
        "      out.append('min, and')\n",
        "    else:\n",
        "      out.append('mins, and')\n",
        "  out.append(f'{s:.2f}')\n",
        "  out.append('secs')\n",
        "  return ' '.join(out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XPoK-tAHJqaR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 4. Run Training Loop"
      ]
    },
    {
      "metadata": {
        "id": "kRB1G3CCG6Lt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Set hyperparameters\n",
        "MAX_LEN = 32\n",
        "BATCH_SIZE = 128\n",
        "EPOCHS = 20\n",
        "x_dir = data_dir + 'train_input.csv'\n",
        "y_dir = data_dir + 'train_output.csv'\n",
        "NORM_LIM = 3 # value for clip_norm\n",
        "\n",
        "# Load data\n",
        "train_buckets, test_buckets, tkzr, seqs, train_steps, val_steps = load_dataset(x_dir, y_dir, MAX_LEN, BATCH_SIZE, drop = True)\n",
        "\n",
        "# Save preprocessed training data\n",
        "with open(data_dir + \"/\" + date.isoformat(date.today()) + \"-training-data-and-tkzr.pickle\", 'wb') as f:\n",
        "  p.dump((tkzr, seqs, train_steps, val_steps), f)\n",
        "\n",
        "# Get vocab size\n",
        "num_chars = len(tkzr.word_index) + 1 # Add one for padding\n",
        "embedding_dim = 25\n",
        "units = 50\n",
        "\n",
        "# Define model(s)\n",
        "encoder = Encoder(num_chars, embedding_dim, units, batch_sz = BATCH_SIZE)\n",
        "decoder = Decoder(num_chars, embedding_dim, units, batch_sz = BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "E1QIHlHOHOL5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "checkpoint_dir = data_dir + 'checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WgYulu90Igu7",
        "colab_type": "code",
        "outputId": "884a1510-623e-425f-92c3-5820720a3bf5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 6817
        }
      },
      "cell_type": "code",
      "source": [
        "# Loop over epochs\n",
        "for epoch in range(EPOCHS):\n",
        "  print(f'Starting Epoch {epoch + 1}\\n')\n",
        "  \n",
        "  train_loss.reset_states()\n",
        "  train_acc.reset_states()\n",
        "  val_loss.reset_states()\n",
        "  val_acc.reset_states()\n",
        "  \n",
        "  start = time.time()\n",
        "  \n",
        "  total_batches = 0\n",
        "  val_batches = 0\n",
        "\n",
        "  # Loop over buckets\n",
        "  for bucket, dataset in enumerate(train_buckets):\n",
        "    # Reset hidden state\n",
        "    enc_hidden = encoder.initialize_hidden_state()\n",
        "\n",
        "\n",
        "    for inp, targ in dataset.take(-1):\n",
        "      loss, acc = train_step(inp, targ, enc_hidden, NORM_LIM)\n",
        "      \n",
        "      total_batches += 1\n",
        "    \n",
        "      if total_batches % 250 == 0:\n",
        "          print(f'Epoch {epoch + 1} Bucket {bucket + 1}: Loss {loss:.4f}, Acc {acc:.4f} after {total_batches} batches')\n",
        "  \n",
        "  # saving (checkpoint) the model every 2 epochs\n",
        "  if (epoch + 1) % 2 == 0:\n",
        "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "    \n",
        "  # Calculate validation loss and accuracy\n",
        "  for dataset in test_buckets:\n",
        "    # Reset hidden state\n",
        "    enc_hidden = encoder.initialize_hidden_state()\n",
        "    \n",
        "    for inp, targ in dataset.take(-1):\n",
        "      val_loss_, val_acc_ = val_step(inp, targ, enc_hidden)\n",
        "      \n",
        "      val_batches += 1\n",
        "\n",
        "  print(f'\\nEpoch {epoch + 1} Loss {loss:.2f}, Avg Acc {acc*100:.2f}%.')\n",
        "  print(f'Tested on {val_batches * BATCH_SIZE} validation examples:')\n",
        "  print(f'val_loss = {val_loss_:.2f} val_acc = {val_acc_*100:.2f}%')\n",
        "  print(f'Time taken for 1 epoch: {format_time(time.time() - start)}\\n===========================\\n\\n')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting Epoch 1\n",
            "\n",
            "Epoch 1 Bucket 15: Loss 100.2790, Acc 0.1914 after 250 batches\n",
            "Epoch 1 Bucket 21: Loss 90.4950, Acc 0.2380 after 500 batches\n",
            "Epoch 1 Bucket 25: Loss 85.5238, Acc 0.2640 after 750 batches\n",
            "Epoch 1 Bucket 28: Loss 82.4062, Acc 0.2824 after 1000 batches\n",
            "Epoch 1 Bucket 31: Loss 80.3263, Acc 0.2964 after 1250 batches\n",
            "Epoch 1 Bucket 32: Loss 78.8036, Acc 0.3076 after 1500 batches\n",
            "Epoch 1 Bucket 34: Loss 77.5342, Acc 0.3171 after 1750 batches\n",
            "Epoch 1 Bucket 35: Loss 76.6332, Acc 0.3246 after 2000 batches\n",
            "Epoch 1 Bucket 36: Loss 75.8023, Acc 0.3311 after 2250 batches\n",
            "Epoch 1 Bucket 37: Loss 75.2700, Acc 0.3359 after 2500 batches\n",
            "\n",
            "Epoch 1 Loss 75.12, Avg Acc 33.76%.\n",
            "Tested on 63616 validation examples:\n",
            "val_loss = 67.67 val_acc = 39.05%\n",
            "Time taken for 1 epoch: 28 mins, and 21.75 secs\n",
            "===========================\n",
            "\n",
            "\n",
            "Starting Epoch 2\n",
            "\n",
            "Epoch 2 Bucket 15: Loss 63.9322, Acc 0.4054 after 250 batches\n",
            "Epoch 2 Bucket 21: Loss 63.9222, Acc 0.4100 after 500 batches\n",
            "Epoch 2 Bucket 25: Loss 63.8943, Acc 0.4124 after 750 batches\n",
            "Epoch 2 Bucket 28: Loss 63.8732, Acc 0.4136 after 1000 batches\n",
            "Epoch 2 Bucket 31: Loss 64.0302, Acc 0.4138 after 1250 batches\n",
            "Epoch 2 Bucket 32: Loss 64.2351, Acc 0.4138 after 1500 batches\n",
            "Epoch 2 Bucket 34: Loss 64.3385, Acc 0.4142 after 1750 batches\n",
            "Epoch 2 Bucket 35: Loss 64.5545, Acc 0.4138 after 2000 batches\n",
            "Epoch 2 Bucket 36: Loss 64.6425, Acc 0.4139 after 2250 batches\n",
            "Epoch 2 Bucket 37: Loss 64.8945, Acc 0.4133 after 2500 batches\n",
            "\n",
            "Epoch 2 Loss 65.07, Avg Acc 41.28%.\n",
            "Tested on 63616 validation examples:\n",
            "val_loss = 63.97 val_acc = 42.31%\n",
            "Time taken for 1 epoch: 24 mins, and 35.92 secs\n",
            "===========================\n",
            "\n",
            "\n",
            "Starting Epoch 3\n",
            "\n",
            "Epoch 3 Bucket 15: Loss 59.7091, Acc 0.4455 after 250 batches\n",
            "Epoch 3 Bucket 21: Loss 59.9614, Acc 0.4475 after 500 batches\n",
            "Epoch 3 Bucket 25: Loss 60.1390, Acc 0.4478 after 750 batches\n",
            "Epoch 3 Bucket 28: Loss 60.2922, Acc 0.4474 after 1000 batches\n",
            "Epoch 3 Bucket 31: Loss 60.6119, Acc 0.4462 after 1250 batches\n",
            "Epoch 3 Bucket 32: Loss 60.9618, Acc 0.4447 after 1500 batches\n",
            "Epoch 3 Bucket 34: Loss 61.2195, Acc 0.4436 after 1750 batches\n",
            "Epoch 3 Bucket 35: Loss 61.5608, Acc 0.4421 after 2000 batches\n",
            "Epoch 3 Bucket 36: Loss 61.7711, Acc 0.4410 after 2250 batches\n",
            "Epoch 3 Bucket 37: Loss 62.1406, Acc 0.4393 after 2500 batches\n",
            "\n",
            "Epoch 3 Loss 62.33, Avg Acc 43.87%.\n",
            "Tested on 63616 validation examples:\n",
            "val_loss = 61.75 val_acc = 44.40%\n",
            "Time taken for 1 epoch: 25 mins, and 8.52 secs\n",
            "===========================\n",
            "\n",
            "\n",
            "Starting Epoch 4\n",
            "\n",
            "Epoch 4 Bucket 15: Loss 56.9332, Acc 0.4723 after 250 batches\n",
            "Epoch 4 Bucket 21: Loss 57.3613, Acc 0.4725 after 500 batches\n",
            "Epoch 4 Bucket 25: Loss 57.6536, Acc 0.4717 after 750 batches\n",
            "Epoch 4 Bucket 28: Loss 57.8919, Acc 0.4706 after 1000 batches\n",
            "Epoch 4 Bucket 31: Loss 58.2780, Acc 0.4689 after 1250 batches\n",
            "Epoch 4 Bucket 32: Loss 58.6702, Acc 0.4674 after 1500 batches\n",
            "Epoch 4 Bucket 34: Loss 58.9859, Acc 0.4661 after 1750 batches\n",
            "Epoch 4 Bucket 35: Loss 59.3247, Acc 0.4651 after 2000 batches\n",
            "Epoch 4 Bucket 36: Loss 59.5412, Acc 0.4644 after 2250 batches\n",
            "Epoch 4 Bucket 37: Loss 59.9170, Acc 0.4630 after 2500 batches\n",
            "\n",
            "Epoch 4 Loss 60.00, Avg Acc 46.35%.\n",
            "Tested on 63616 validation examples:\n",
            "val_loss = 58.19 val_acc = 48.77%\n",
            "Time taken for 1 epoch: 25 mins, and 7.55 secs\n",
            "===========================\n",
            "\n",
            "\n",
            "Starting Epoch 5\n",
            "\n",
            "Epoch 5 Bucket 15: Loss 50.4987, Acc 0.5466 after 250 batches\n",
            "Epoch 5 Bucket 21: Loss 49.9622, Acc 0.5580 after 500 batches\n",
            "Epoch 5 Bucket 25: Loss 48.9176, Acc 0.5719 after 750 batches\n",
            "Epoch 5 Bucket 28: Loss 46.8970, Acc 0.5953 after 1000 batches\n",
            "Epoch 5 Bucket 31: Loss 45.2994, Acc 0.6120 after 1250 batches\n",
            "Epoch 5 Bucket 32: Loss 44.2352, Acc 0.6225 after 1500 batches\n",
            "Epoch 5 Bucket 34: Loss 43.9206, Acc 0.6254 after 1750 batches\n",
            "Epoch 5 Bucket 35: Loss 43.5054, Acc 0.6294 after 2000 batches\n",
            "Epoch 5 Bucket 36: Loss 43.4213, Acc 0.6298 after 2250 batches\n",
            "Epoch 5 Bucket 37: Loss 43.6637, Acc 0.6280 after 2500 batches\n",
            "\n",
            "Epoch 5 Loss 43.06, Avg Acc 63.38%.\n",
            "Tested on 63616 validation examples:\n",
            "val_loss = 32.34 val_acc = 73.25%\n",
            "Time taken for 1 epoch: 25 mins, and 26.29 secs\n",
            "===========================\n",
            "\n",
            "\n",
            "Starting Epoch 6\n",
            "\n",
            "Epoch 6 Bucket 15: Loss 23.0313, Acc 0.8118 after 250 batches\n",
            "Epoch 6 Bucket 21: Loss 24.6636, Acc 0.7960 after 500 batches\n",
            "Epoch 6 Bucket 25: Loss 25.5492, Acc 0.7868 after 750 batches\n",
            "Epoch 6 Bucket 28: Loss 26.3314, Acc 0.7790 after 1000 batches\n",
            "Epoch 6 Bucket 31: Loss 27.3220, Acc 0.7697 after 1250 batches\n",
            "Epoch 6 Bucket 32: Loss 28.3551, Acc 0.7601 after 1500 batches\n",
            "Epoch 6 Bucket 34: Loss 29.7576, Acc 0.7472 after 1750 batches\n",
            "Epoch 6 Bucket 35: Loss 30.6936, Acc 0.7389 after 2000 batches\n",
            "Epoch 6 Bucket 36: Loss 31.7376, Acc 0.7291 after 2250 batches\n",
            "Epoch 6 Bucket 37: Loss 32.9202, Acc 0.7189 after 2500 batches\n",
            "\n",
            "Epoch 6 Loss 32.69, Avg Acc 72.13%.\n",
            "Tested on 63616 validation examples:\n",
            "val_loss = 30.19 val_acc = 74.64%\n",
            "Time taken for 1 epoch: 25 mins, and 13.84 secs\n",
            "===========================\n",
            "\n",
            "\n",
            "Starting Epoch 7\n",
            "\n",
            "Epoch 7 Bucket 15: Loss 21.1702, Acc 0.8255 after 250 batches\n",
            "Epoch 7 Bucket 21: Loss 22.8511, Acc 0.8094 after 500 batches\n",
            "Epoch 7 Bucket 25: Loss 23.7949, Acc 0.7997 after 750 batches\n",
            "Epoch 7 Bucket 28: Loss 24.6173, Acc 0.7915 after 1000 batches\n",
            "Epoch 7 Bucket 31: Loss 25.6496, Acc 0.7818 after 1250 batches\n",
            "Epoch 7 Bucket 32: Loss 26.7390, Acc 0.7718 after 1500 batches\n",
            "Epoch 7 Bucket 34: Loss 28.1986, Acc 0.7585 after 1750 batches\n",
            "Epoch 7 Bucket 35: Loss 29.1858, Acc 0.7498 after 2000 batches\n",
            "Epoch 7 Bucket 36: Loss 30.2816, Acc 0.7396 after 2250 batches\n",
            "Epoch 7 Bucket 37: Loss 31.5119, Acc 0.7290 after 2500 batches\n",
            "\n",
            "Epoch 7 Loss 31.30, Avg Acc 73.13%.\n",
            "Tested on 63616 validation examples:\n",
            "val_loss = 29.33 val_acc = 75.33%\n",
            "Time taken for 1 epoch: 25 mins, and 6.50 secs\n",
            "===========================\n",
            "\n",
            "\n",
            "Starting Epoch 8\n",
            "\n",
            "Epoch 8 Bucket 15: Loss 20.3563, Acc 0.8315 after 250 batches\n",
            "Epoch 8 Bucket 21: Loss 22.0413, Acc 0.8153 after 500 batches\n",
            "Epoch 8 Bucket 25: Loss 23.0069, Acc 0.8056 after 750 batches\n",
            "Epoch 8 Bucket 28: Loss 23.8374, Acc 0.7975 after 1000 batches\n",
            "Epoch 8 Bucket 31: Loss 24.8853, Acc 0.7877 after 1250 batches\n",
            "Epoch 8 Bucket 32: Loss 25.9931, Acc 0.7775 after 1500 batches\n",
            "Epoch 8 Bucket 34: Loss 27.4692, Acc 0.7641 after 1750 batches\n",
            "Epoch 8 Bucket 35: Loss 28.4754, Acc 0.7552 after 2000 batches\n",
            "Epoch 8 Bucket 36: Loss 29.5795, Acc 0.7449 after 2250 batches\n",
            "Epoch 8 Bucket 37: Loss 30.8268, Acc 0.7342 after 2500 batches\n",
            "\n",
            "Epoch 8 Loss 30.61, Avg Acc 73.65%.\n",
            "Tested on 63616 validation examples:\n",
            "val_loss = 28.81 val_acc = 75.73%\n",
            "Time taken for 1 epoch: 25 mins, and 44.52 secs\n",
            "===========================\n",
            "\n",
            "\n",
            "Starting Epoch 9\n",
            "\n",
            "Epoch 9 Bucket 15: Loss 19.7778, Acc 0.8364 after 250 batches\n",
            "Epoch 9 Bucket 21: Loss 21.4577, Acc 0.8203 after 500 batches\n",
            "Epoch 9 Bucket 25: Loss 22.4265, Acc 0.8105 after 750 batches\n",
            "Epoch 9 Bucket 28: Loss 23.2656, Acc 0.8024 after 1000 batches\n",
            "Epoch 9 Bucket 31: Loss 24.3046, Acc 0.7926 after 1250 batches\n",
            "Epoch 9 Bucket 32: Loss 25.4118, Acc 0.7824 after 1500 batches\n",
            "Epoch 9 Bucket 34: Loss 26.8929, Acc 0.7690 after 1750 batches\n",
            "Epoch 9 Bucket 35: Loss 27.9031, Acc 0.7601 after 2000 batches\n",
            "Epoch 9 Bucket 36: Loss 29.0116, Acc 0.7497 after 2250 batches\n",
            "Epoch 9 Bucket 37: Loss 30.2626, Acc 0.7390 after 2500 batches\n",
            "\n",
            "Epoch 9 Loss 30.05, Avg Acc 74.12%.\n",
            "Tested on 63616 validation examples:\n",
            "val_loss = 28.04 val_acc = 76.27%\n",
            "Time taken for 1 epoch: 25 mins, and 42.83 secs\n",
            "===========================\n",
            "\n",
            "\n",
            "Starting Epoch 10\n",
            "\n",
            "Epoch 10 Bucket 15: Loss 19.2924, Acc 0.8405 after 250 batches\n",
            "Epoch 10 Bucket 21: Loss 20.9703, Acc 0.8244 after 500 batches\n",
            "Epoch 10 Bucket 25: Loss 21.9316, Acc 0.8147 after 750 batches\n",
            "Epoch 10 Bucket 28: Loss 22.7717, Acc 0.8065 after 1000 batches\n",
            "Epoch 10 Bucket 31: Loss 23.8079, Acc 0.7968 after 1250 batches\n",
            "Epoch 10 Bucket 32: Loss 24.9110, Acc 0.7866 after 1500 batches\n",
            "Epoch 10 Bucket 34: Loss 26.4017, Acc 0.7731 after 1750 batches\n",
            "Epoch 10 Bucket 35: Loss 27.4160, Acc 0.7642 after 2000 batches\n",
            "Epoch 10 Bucket 36: Loss 28.5350, Acc 0.7538 after 2250 batches\n",
            "Epoch 10 Bucket 37: Loss 29.7981, Acc 0.7429 after 2500 batches\n",
            "\n",
            "Epoch 10 Loss 29.58, Avg Acc 74.51%.\n",
            "Tested on 63616 validation examples:\n",
            "val_loss = 27.76 val_acc = 76.60%\n",
            "Time taken for 1 epoch: 25 mins, and 3.13 secs\n",
            "===========================\n",
            "\n",
            "\n",
            "Starting Epoch 11\n",
            "\n",
            "Epoch 11 Bucket 15: Loss 18.9040, Acc 0.8437 after 250 batches\n",
            "Epoch 11 Bucket 21: Loss 20.5742, Acc 0.8276 after 500 batches\n",
            "Epoch 11 Bucket 25: Loss 21.5393, Acc 0.8180 after 750 batches\n",
            "Epoch 11 Bucket 28: Loss 22.3847, Acc 0.8097 after 1000 batches\n",
            "Epoch 11 Bucket 31: Loss 23.4281, Acc 0.7998 after 1250 batches\n",
            "Epoch 11 Bucket 32: Loss 24.5326, Acc 0.7897 after 1500 batches\n",
            "Epoch 11 Bucket 34: Loss 26.0237, Acc 0.7762 after 1750 batches\n",
            "Epoch 11 Bucket 35: Loss 27.0420, Acc 0.7672 after 2000 batches\n",
            "Epoch 11 Bucket 36: Loss 28.1609, Acc 0.7568 after 2250 batches\n",
            "Epoch 11 Bucket 37: Loss 29.4203, Acc 0.7460 after 2500 batches\n",
            "\n",
            "Epoch 11 Loss 29.21, Avg Acc 74.82%.\n",
            "Tested on 63616 validation examples:\n",
            "val_loss = 27.54 val_acc = 76.77%\n",
            "Time taken for 1 epoch: 22 mins, and 17.31 secs\n",
            "===========================\n",
            "\n",
            "\n",
            "Starting Epoch 12\n",
            "\n",
            "Epoch 12 Bucket 15: Loss 18.5764, Acc 0.8463 after 250 batches\n",
            "Epoch 12 Bucket 21: Loss 20.2536, Acc 0.8301 after 500 batches\n",
            "Epoch 12 Bucket 25: Loss 21.2141, Acc 0.8205 after 750 batches\n",
            "Epoch 12 Bucket 28: Loss 22.0535, Acc 0.8123 after 1000 batches\n",
            "Epoch 12 Bucket 31: Loss 23.0984, Acc 0.8024 after 1250 batches\n",
            "Epoch 12 Bucket 32: Loss 24.2040, Acc 0.7923 after 1500 batches\n",
            "Epoch 12 Bucket 34: Loss 25.6997, Acc 0.7788 after 1750 batches\n",
            "Epoch 12 Bucket 35: Loss 26.7130, Acc 0.7698 after 2000 batches\n",
            "Epoch 12 Bucket 36: Loss 27.8387, Acc 0.7594 after 2250 batches\n",
            "Epoch 12 Bucket 37: Loss 29.1041, Acc 0.7485 after 2500 batches\n",
            "\n",
            "Epoch 12 Loss 28.89, Avg Acc 75.07%.\n",
            "Tested on 63616 validation examples:\n",
            "val_loss = 27.18 val_acc = 77.02%\n",
            "Time taken for 1 epoch: 21 mins, and 37.69 secs\n",
            "===========================\n",
            "\n",
            "\n",
            "Starting Epoch 13\n",
            "\n",
            "Epoch 13 Bucket 15: Loss 18.3171, Acc 0.8483 after 250 batches\n",
            "Epoch 13 Bucket 21: Loss 19.9832, Acc 0.8322 after 500 batches\n",
            "Epoch 13 Bucket 25: Loss 20.9433, Acc 0.8226 after 750 batches\n",
            "Epoch 13 Bucket 28: Loss 21.7867, Acc 0.8143 after 1000 batches\n",
            "Epoch 13 Bucket 31: Loss 22.8301, Acc 0.8046 after 1250 batches\n",
            "Epoch 13 Bucket 32: Loss 23.9354, Acc 0.7944 after 1500 batches\n",
            "Epoch 13 Bucket 34: Loss 25.4332, Acc 0.7809 after 1750 batches\n",
            "Epoch 13 Bucket 35: Loss 26.4481, Acc 0.7720 after 2000 batches\n",
            "Epoch 13 Bucket 36: Loss 27.5745, Acc 0.7615 after 2250 batches\n",
            "Epoch 13 Bucket 37: Loss 28.8423, Acc 0.7507 after 2500 batches\n",
            "\n",
            "Epoch 13 Loss 28.63, Avg Acc 75.28%.\n",
            "Tested on 63616 validation examples:\n",
            "val_loss = 26.73 val_acc = 77.25%\n",
            "Time taken for 1 epoch: 21 mins, and 31.42 secs\n",
            "===========================\n",
            "\n",
            "\n",
            "Starting Epoch 14\n",
            "\n",
            "Epoch 14 Bucket 15: Loss 18.1035, Acc 0.8498 after 250 batches\n",
            "Epoch 14 Bucket 21: Loss 19.7615, Acc 0.8337 after 500 batches\n",
            "Epoch 14 Bucket 25: Loss 20.7249, Acc 0.8241 after 750 batches\n",
            "Epoch 14 Bucket 28: Loss 21.5703, Acc 0.8158 after 1000 batches\n",
            "Epoch 14 Bucket 31: Loss 22.6159, Acc 0.8061 after 1250 batches\n",
            "Epoch 14 Bucket 32: Loss 23.7247, Acc 0.7960 after 1500 batches\n",
            "Epoch 14 Bucket 34: Loss 25.2190, Acc 0.7826 after 1750 batches\n",
            "Epoch 14 Bucket 35: Loss 26.2322, Acc 0.7737 after 2000 batches\n",
            "Epoch 14 Bucket 36: Loss 27.3575, Acc 0.7633 after 2250 batches\n",
            "Epoch 14 Bucket 37: Loss 28.6236, Acc 0.7524 after 2500 batches\n",
            "\n",
            "Epoch 14 Loss 28.41, Avg Acc 75.46%.\n",
            "Tested on 63616 validation examples:\n",
            "val_loss = 26.61 val_acc = 77.37%\n",
            "Time taken for 1 epoch: 21 mins, and 39.14 secs\n",
            "===========================\n",
            "\n",
            "\n",
            "Starting Epoch 15\n",
            "\n",
            "Epoch 15 Bucket 15: Loss 17.9280, Acc 0.8510 after 250 batches\n",
            "Epoch 15 Bucket 21: Loss 19.5918, Acc 0.8349 after 500 batches\n",
            "Epoch 15 Bucket 25: Loss 20.5526, Acc 0.8254 after 750 batches\n",
            "Epoch 15 Bucket 28: Loss 21.3973, Acc 0.8172 after 1000 batches\n",
            "Epoch 15 Bucket 31: Loss 22.4394, Acc 0.8074 after 1250 batches\n",
            "Epoch 15 Bucket 32: Loss 23.5477, Acc 0.7973 after 1500 batches\n",
            "Epoch 15 Bucket 34: Loss 25.0409, Acc 0.7839 after 1750 batches\n",
            "Epoch 15 Bucket 35: Loss 26.0582, Acc 0.7750 after 2000 batches\n",
            "Epoch 15 Bucket 36: Loss 27.1831, Acc 0.7646 after 2250 batches\n",
            "Epoch 15 Bucket 37: Loss 28.4486, Acc 0.7538 after 2500 batches\n",
            "\n",
            "Epoch 15 Loss 28.24, Avg Acc 75.60%.\n",
            "Tested on 63616 validation examples:\n",
            "val_loss = 26.40 val_acc = 77.58%\n",
            "Time taken for 1 epoch: 21 mins, and 20.11 secs\n",
            "===========================\n",
            "\n",
            "\n",
            "Starting Epoch 16\n",
            "\n",
            "Epoch 16 Bucket 15: Loss 17.7886, Acc 0.8521 after 250 batches\n",
            "Epoch 16 Bucket 21: Loss 19.4468, Acc 0.8361 after 500 batches\n",
            "Epoch 16 Bucket 25: Loss 20.4160, Acc 0.8264 after 750 batches\n",
            "Epoch 16 Bucket 28: Loss 21.2530, Acc 0.8183 after 1000 batches\n",
            "Epoch 16 Bucket 31: Loss 22.2989, Acc 0.8086 after 1250 batches\n",
            "Epoch 16 Bucket 32: Loss 23.4095, Acc 0.7985 after 1500 batches\n",
            "Epoch 16 Bucket 34: Loss 24.8986, Acc 0.7851 after 1750 batches\n",
            "Epoch 16 Bucket 35: Loss 25.9128, Acc 0.7762 after 2000 batches\n",
            "Epoch 16 Bucket 36: Loss 27.0371, Acc 0.7658 after 2250 batches\n",
            "Epoch 16 Bucket 37: Loss 28.3005, Acc 0.7550 after 2500 batches\n",
            "\n",
            "Epoch 16 Loss 28.09, Avg Acc 75.72%.\n",
            "Tested on 63616 validation examples:\n",
            "val_loss = 26.23 val_acc = 77.70%\n",
            "Time taken for 1 epoch: 20 mins, and 34.38 secs\n",
            "===========================\n",
            "\n",
            "\n",
            "Starting Epoch 17\n",
            "\n",
            "Epoch 17 Bucket 15: Loss 17.6617, Acc 0.8531 after 250 batches\n",
            "Epoch 17 Bucket 21: Loss 19.3275, Acc 0.8370 after 500 batches\n",
            "Epoch 17 Bucket 25: Loss 20.2997, Acc 0.8273 after 750 batches\n",
            "Epoch 17 Bucket 28: Loss 21.1379, Acc 0.8192 after 1000 batches\n",
            "Epoch 17 Bucket 31: Loss 22.1805, Acc 0.8095 after 1250 batches\n",
            "Epoch 17 Bucket 32: Loss 23.2923, Acc 0.7994 after 1500 batches\n",
            "Epoch 17 Bucket 34: Loss 24.7774, Acc 0.7861 after 1750 batches\n",
            "Epoch 17 Bucket 35: Loss 25.7892, Acc 0.7772 after 2000 batches\n",
            "Epoch 17 Bucket 36: Loss 26.9131, Acc 0.7669 after 2250 batches\n",
            "Epoch 17 Bucket 37: Loss 28.1732, Acc 0.7561 after 2500 batches\n",
            "\n",
            "Epoch 17 Loss 27.97, Avg Acc 75.82%.\n",
            "Tested on 63616 validation examples:\n",
            "val_loss = 26.15 val_acc = 77.80%\n",
            "Time taken for 1 epoch: 19 mins, and 22.53 secs\n",
            "===========================\n",
            "\n",
            "\n",
            "Starting Epoch 18\n",
            "\n",
            "Epoch 18 Bucket 15: Loss 17.5605, Acc 0.8537 after 250 batches\n",
            "Epoch 18 Bucket 21: Loss 19.2237, Acc 0.8378 after 500 batches\n",
            "Epoch 18 Bucket 25: Loss 20.1925, Acc 0.8282 after 750 batches\n",
            "Epoch 18 Bucket 28: Loss 21.0310, Acc 0.8201 after 1000 batches\n",
            "Epoch 18 Bucket 31: Loss 22.0697, Acc 0.8104 after 1250 batches\n",
            "Epoch 18 Bucket 32: Loss 23.1778, Acc 0.8003 after 1500 batches\n",
            "Epoch 18 Bucket 34: Loss 24.6645, Acc 0.7870 after 1750 batches\n",
            "Epoch 18 Bucket 35: Loss 25.6793, Acc 0.7782 after 2000 batches\n",
            "Epoch 18 Bucket 36: Loss 26.7997, Acc 0.7678 after 2250 batches\n",
            "Epoch 18 Bucket 37: Loss 28.0582, Acc 0.7571 after 2500 batches\n",
            "\n",
            "Epoch 18 Loss 27.85, Avg Acc 75.92%.\n",
            "Tested on 63616 validation examples:\n",
            "val_loss = 26.21 val_acc = 77.80%\n",
            "Time taken for 1 epoch: 19 mins, and 47.57 secs\n",
            "===========================\n",
            "\n",
            "\n",
            "Starting Epoch 19\n",
            "\n",
            "Epoch 19 Bucket 15: Loss 17.4965, Acc 0.8543 after 250 batches\n",
            "Epoch 19 Bucket 21: Loss 19.1481, Acc 0.8384 after 500 batches\n",
            "Epoch 19 Bucket 25: Loss 20.1144, Acc 0.8288 after 750 batches\n",
            "Epoch 19 Bucket 28: Loss 20.9530, Acc 0.8207 after 1000 batches\n",
            "Epoch 19 Bucket 31: Loss 21.9827, Acc 0.8111 after 1250 batches\n",
            "Epoch 19 Bucket 32: Loss 23.0893, Acc 0.8011 after 1500 batches\n",
            "Epoch 19 Bucket 34: Loss 24.5721, Acc 0.7878 after 1750 batches\n",
            "Epoch 19 Bucket 35: Loss 25.5833, Acc 0.7790 after 2000 batches\n",
            "Epoch 19 Bucket 36: Loss 26.7058, Acc 0.7687 after 2250 batches\n",
            "Epoch 19 Bucket 37: Loss 27.9644, Acc 0.7579 after 2500 batches\n",
            "\n",
            "Epoch 19 Loss 27.76, Avg Acc 76.00%.\n",
            "Tested on 63616 validation examples:\n",
            "val_loss = 26.02 val_acc = 77.92%\n",
            "Time taken for 1 epoch: 19 mins, and 30.33 secs\n",
            "===========================\n",
            "\n",
            "\n",
            "Starting Epoch 20\n",
            "\n",
            "Epoch 20 Bucket 15: Loss 17.4166, Acc 0.8550 after 250 batches\n",
            "Epoch 20 Bucket 21: Loss 19.0660, Acc 0.8392 after 500 batches\n",
            "Epoch 20 Bucket 25: Loss 20.0301, Acc 0.8296 after 750 batches\n",
            "Epoch 20 Bucket 28: Loss 20.8669, Acc 0.8214 after 1000 batches\n",
            "Epoch 20 Bucket 31: Loss 21.8944, Acc 0.8119 after 1250 batches\n",
            "Epoch 20 Bucket 32: Loss 22.9981, Acc 0.8019 after 1500 batches\n",
            "Epoch 20 Bucket 34: Loss 24.4796, Acc 0.7886 after 1750 batches\n",
            "Epoch 20 Bucket 35: Loss 25.4877, Acc 0.7798 after 2000 batches\n",
            "Epoch 20 Bucket 36: Loss 26.6056, Acc 0.7696 after 2250 batches\n",
            "Epoch 20 Bucket 37: Loss 27.8612, Acc 0.7588 after 2500 batches\n",
            "\n",
            "Epoch 20 Loss 27.66, Avg Acc 76.09%.\n",
            "Tested on 63616 validation examples:\n",
            "val_loss = 25.92 val_acc = 77.99%\n",
            "Time taken for 1 epoch: 18 mins, and 57.32 secs\n",
            "===========================\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}