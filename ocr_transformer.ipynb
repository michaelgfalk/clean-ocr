{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ocr_transformer.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/michaelgfalk/clean-ocr/blob/master/ocr_transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "XQdbN2CEIgt5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Waves of Words: Correcting Trove's Messy OCR\n",
        "\n",
        "One aim of the *Waves of Words* project is to extract Aboriginal wordlists from [Trove](https://trove.nla.gov.au). A challenge we face is that historical newspapers are difficult to OCR, so many of the texts are riddled with errors.\n",
        "\n",
        "Using the training data available from the ALTA 2017 OCR competition, can we create a model that will clean the text enough for our aboriginal word detector to work?\n",
        "\n",
        "I have been giving some thought to whether uppercase letters and punctuation should be preserved in this model, given that the aim is to clean up the text for our detector, which only requires lower case letters and ignores punctuation. I think we need to include all the characters in this one. The extra information about sentence barriers, for example, should hopefully help the model as it would a human when it tries to correct the text. Moreover, many OCR errors involve exchaning punctuation or digits for letters, e.g. `l = 1 = !`.\n",
        "\n",
        "**References:**\n",
        "\n",
        "* D. Mollá, S. Cassidy. Overview of the 2017 ALTA Shared Task:\n",
        "Correcting OCR Errors (2017). *Proc. ALTA 2017*.\n",
        "[https://aclanthology.coli.uni-saarland.de/papers/U17-1014/u17-1014](https://aclanthology.coli.uni-saarland.de/papers/U17-1014/u17-1014)"
      ]
    },
    {
      "metadata": {
        "id": "PzfYL5jcnT7m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Install TensorFlow2\n",
        "!pip install -q tensorflow-gpu==2.0.0-alpha0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ePS0gU47IW-u",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "import csv\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-V1HqBKxgqry",
        "colab_type": "code",
        "outputId": "c4872172-88b6-44fe-b25b-af344e56b79e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "# Mount google drive to get training data. Set data_dir\n",
        "drive.mount('/content/gdrive')\n",
        "data_dir = '/content/gdrive/My Drive/waves_of_words/ocr_correction_data/'"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Yur9PBcBzjBp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 1. Data Pipeline (new)\n",
        "\n",
        "This new data pipeline makes use of Tensorflows Dataset object to manage the extraction and transformation more efficiently."
      ]
    },
    {
      "metadata": {
        "id": "-JwKCURv0I44",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_dataset(source, target, start = 'स', end = 'ए'):\n",
        "  \"\"\"Imports articles from csv and packages for training.\"\"\"\n",
        "  \n",
        "  # Lists for import\n",
        "  raw_x = []\n",
        "  raw_y = []\n",
        "  \n",
        "  # Import raw text\n",
        "  with open(source, \"rt\") as f:\n",
        "    x_reader = csv.reader(f, delimiter = ',', quotechar = '\"')\n",
        "    for row in x_reader:\n",
        "      raw_x.append(row[1])\n",
        "  with open(target, \"rt\") as f:\n",
        "    y_reader = csv.reader(f, delimiter = ',', quotechar = '\"')\n",
        "    for row in y_reader:\n",
        "      raw_y.append(row[1])\n",
        "  \n",
        "  # Drop header rows\n",
        "  raw_x = raw_x[1:]\n",
        "  raw_y = raw_y[1:]\n",
        "  \n",
        "  # Add special characters\n",
        "  x = [start + article + end for article in raw_x if start not in article and end not in article]\n",
        "  y = [start + article + end for article in raw_y if start not in article and end not in article]\n",
        "  \n",
        "  return x, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_KIK6ErA3rf5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def tokenize(source, target):\n",
        "  \"\"\"Instantiates tokenizer, fits and applies.\"\"\"\n",
        "  \n",
        "  # Fit tokenizer\n",
        "  tkzr = tf.keras.preprocessing.text.Tokenizer(\n",
        "    num_words = None,\n",
        "    filters = None,\n",
        "    lower = False,\n",
        "    char_level = True\n",
        "  )\n",
        "  tkzr.fit_on_texts(source + target)\n",
        "  \n",
        "  # Apply to texts\n",
        "  x = tkzr.texts_to_sequences(source)\n",
        "  y = tkzr.texts_to_sequences(target)\n",
        "  \n",
        "  return x, y, tkzr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vO17zE6HPV-n",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def split_and_stack(x, y, max_len, batch_size, drop = False):\n",
        "  \"\"\"Takes as input two python lists, and outputs a list of tensor buckets.\n",
        "  \n",
        "  Arguments:\n",
        "  ==========\n",
        "  x (list): the tokenized source strings\n",
        "  y (list): the tokenized target strings\n",
        "  max_len (int): the maximum number of time steps the model will consider\n",
        "  batch_size (int): the batch size for the training examples\n",
        "  drop (bool): keep the final batch, if len(x) is not a multiple of batch_size?\n",
        "  \n",
        "  Returns:\n",
        "  ==========\n",
        "  bucket_list (list): a list of length m, each item of which is a bucket of\n",
        "    similar-length numpy array\n",
        "  \n",
        "  A bucket is a tensor of shape (2, m_prime, max_len). Training examples are first\n",
        "  bucketed into groups of similar length, and seperated into batches of batch_size.\n",
        "  Each batch is then padded out to an integer multiple of max_len, split into chunks\n",
        "  of length max_len and stacked.\n",
        "  \n",
        "  This bucketing, splitting and stacking allows the data to be fed to a stateful RNN.\n",
        "  \n",
        "  Dimensions:\n",
        "  -The first dimension seperates x examples from y\n",
        "  -The second dimension seperates individual training examples.\n",
        "    m_prime = batch_size * ⌈max_len_x_or_y_within_batch / batch_size⌉ \n",
        "  -The third dimension seperates individual time-steps, and is fixed at max_len\"\"\"\n",
        "  \n",
        "  # Set number of training examples (round up if not dropping)\n",
        "  assert len(x) == len(y)\n",
        "  m = len(x)\n",
        "  \n",
        "  # Sort x and y by sequence length\n",
        "  x_y = sorted(zip(x,y), key = lambda tup: max(len(tup[0]), len(tup[1])), reverse = True)\n",
        "  \n",
        "  # Loop through list and create batches\n",
        "  bucket_list = []\n",
        "  for i in range(0, m, batch_size):\n",
        "    \n",
        "    # Slice and unpack the list\n",
        "    bn = batch_size\n",
        "    if len(x_y) < batch_size and drop:\n",
        "      break\n",
        "    elif len(x_y) < batch_size and not drop:\n",
        "      bn = len(x_y)\n",
        "    \n",
        "    bx, by = zip(*[x_y.pop() for _ in range(bn)])\n",
        "    \n",
        "    # Calculate length boundary and m_prime\n",
        "    bl = max(len(bx[0]), len(by[0]))\n",
        "    b = max_len - (bl % max_len) + bl\n",
        "    m_prime = int(b / max_len)\n",
        "    \n",
        "    # Pad the sequences\n",
        "    x_pad = pad_sequences(bx, maxlen = b, padding = 'post')\n",
        "    y_pad = pad_sequences(by, maxlen = b, padding = 'post')\n",
        "    \n",
        "    # Split and stack\n",
        "    x_out = np.concatenate(np.split(x_pad, m_prime, axis = 1), axis = 0)\n",
        "    y_out = np.concatenate(np.split(y_pad, m_prime, axis = 1), axis = 0)\n",
        "    \n",
        "    # Covert to dataset and append to list\n",
        "    bucket = tf.data.Dataset.from_tensor_slices((x_out, y_out))\n",
        "    bucket = bucket.batch(bn) # NB: bn = batch_size except on the last batch, if drop != True\n",
        "    bucket_list.append(bucket)\n",
        "    \n",
        "  return bucket_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0xWguIxG5VHm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_dataset(x_path, y_path, max_len = 20, batch_size = 128, drop = False, test_size = .2):\n",
        "  \n",
        "  x, y = create_dataset(x_path, y_path)\n",
        "  \n",
        "  x, y, tkzr = tokenize(x, y)\n",
        "  \n",
        "  seqs = (x_train, x_test, y_train, y_test) = train_test_split(x, y, test_size = test_size)\n",
        "  \n",
        "  train_buckets = split_and_stack(x_train, y_train, max_len, batch_size, drop)\n",
        "  \n",
        "  test_buckets = split_and_stack(x_test, y_test, max_len, batch_size, drop)\n",
        "  \n",
        "  return train_buckets, test_buckets, tkzr, seqs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IsoTxaMM6Rr4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 2. Model Definition\n",
        "\n",
        "Adapted from TensorFlow docs."
      ]
    },
    {
      "metadata": {
        "id": "lOKqMawV6VJB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, num_chars, embedding_dim, enc_units, batch_sz):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.enc_units = enc_units\n",
        "    self.embedding = tf.keras.layers.Embedding(num_chars, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.enc_units, \n",
        "                                   # The following parameters must be set this way\n",
        "                                   # to use CuDNN on GPU\n",
        "                                   activation='tanh',\n",
        "                                   recurrent_activation='sigmoid',\n",
        "                                   recurrent_dropout=0,\n",
        "                                   unroll=False,\n",
        "                                   use_bias=True,\n",
        "                                   reset_after=True,\n",
        "                                   # The following parameters are necessary for the\n",
        "                                   # encoder-decoder architecture\n",
        "                                   return_sequences=True, \n",
        "                                   return_state=True,\n",
        "                                   # Stateful must be 'True' in order\n",
        "                                   # to link the batches in each hyperbatch\n",
        "                                   stateful=True,\n",
        "                                   # Just the standard initializer\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "\n",
        "  def call(self, x, hidden):\n",
        "    x = self.embedding(x)\n",
        "    output, state = self.gru(x, initial_state = hidden)        \n",
        "    return output, state\n",
        "\n",
        "  def initialize_hidden_state(self):\n",
        "    return tf.zeros((self.batch_sz, self.enc_units))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FDgYbbYn7GiP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class BahdanauAttention(tf.keras.Model):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "  \n",
        "  def call(self, query, values):\n",
        "    # hidden shape == (batch_size, hidden size)\n",
        "    # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "    # we are doing this to perform addition to calculate the score\n",
        "    hidden_with_time_axis = tf.expand_dims(query, 1)\n",
        "\n",
        "    # score shape == (batch_size, max_length, hidden_size)\n",
        "    score = self.V(tf.nn.tanh(\n",
        "        self.W1(values) + self.W2(hidden_with_time_axis)))\n",
        "\n",
        "    # attention_weights shape == (batch_size, max_length, 1)\n",
        "    # we get 1 at the last axis because we are applying score to self.V\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "    # context_vector shape after sum == (batch_size, hidden_size)\n",
        "    context_vector = attention_weights * values\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "    \n",
        "    return context_vector, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yPbOQ2H27a4b",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, num_chars, embedding_dim, dec_units, batch_sz):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.dec_units = dec_units\n",
        "    self.embedding = tf.keras.layers.Embedding(num_chars, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.dec_units, \n",
        "                                   # The following parameters must be set this way\n",
        "                                   # to use CuDNN on GPU\n",
        "                                   activation='tanh',\n",
        "                                   recurrent_activation='sigmoid',\n",
        "                                   recurrent_dropout=0,\n",
        "                                   unroll=False,\n",
        "                                   use_bias=True,\n",
        "                                   reset_after=True,\n",
        "                                   # The following parameters are necessary for the\n",
        "                                   # encoder-decoder architecture\n",
        "                                   return_sequences=True, \n",
        "                                   return_state=True,\n",
        "                                   # Stateful must be 'True' in order\n",
        "                                   # to link the batches in each hyperbatch\n",
        "                                   stateful=True,\n",
        "                                   # Just the standard initializer\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    self.fc = tf.keras.layers.Dense(num_chars)\n",
        "\n",
        "    # used for attention\n",
        "    self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "  def call(self, x, hidden, enc_output):\n",
        "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "\n",
        "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "    # passing the concatenated vector to the GRU\n",
        "    output, state = self.gru(x)\n",
        "\n",
        "    # output shape == (batch_size * 1, hidden_size)\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "    # output shape == (batch_size, vocab)\n",
        "    x = self.fc(output)\n",
        "\n",
        "    return x, state, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-ow7AR1fHKMp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 3. Set up Training"
      ]
    },
    {
      "metadata": {
        "id": "7ta0lQ46HNHb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  # Mask: ignore the model's predictions where the ground truth is padding\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  \n",
        "  # Calculate the loss\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  # Make mask compatible with the loss output\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  \n",
        "  # Multiply the losses by the mask (i.e. zero out all losses where there's just padding)\n",
        "  loss_ *= mask\n",
        "  \n",
        "  return tf.reduce_mean(loss_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IfTMbgXjHRwP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden, norm_lim):\n",
        "  loss = 0\n",
        "        \n",
        "  with tf.GradientTape() as tape:\n",
        "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "\n",
        "    dec_input = tf.expand_dims(inp[:,0], 1)\n",
        "\n",
        "    # Teacher forcing - feeding the target as the next input\n",
        "    for t in range(1, targ.shape[1]):\n",
        "      # passing enc_output to the decoder\n",
        "      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "\n",
        "      loss += loss_function(targ[:, t], predictions)\n",
        "\n",
        "      # using teacher forcing\n",
        "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "  batch_loss = (loss / int(targ.shape[1]))\n",
        "\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "  \n",
        "  # Clip gradients\n",
        "  clipped_gradients = [tf.clip_by_norm(grad, norm_lim) for grad in gradients]\n",
        "\n",
        "  optimizer.apply_gradients(zip(clipped_gradients, variables))\n",
        "  \n",
        "  return batch_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CCSzwR095CVx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def val_step(inp, targ, enc_hidden):\n",
        "  \n",
        "  loss = 0\n",
        "  \n",
        "  # Begin feeding data to network\n",
        "  enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "  dec_hidden = enc_hidden\n",
        "  dec_input = tf.expand_dims(inp[:,0], 1)\n",
        "  \n",
        "  # Cycle through the rest of the time steps\n",
        "  for t in range(1, targ.shape[1]):\n",
        "    # Pass enc_output to the decoder\n",
        "    predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "    loss += loss_function(targ[:,t], predictions)\n",
        "    # Pass the next correct letter to the decoder (teacher forcing)\n",
        "    dec_input = tf.expand_dims(targ[:,t], 1)\n",
        "    \n",
        "  # Calculate val_loss\n",
        "  val_loss = (loss / int(targ.shape[1]))\n",
        "  \n",
        "  return val_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UwilCQ2uVTSO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def format_time(flt):\n",
        "  h = flt//3600\n",
        "  m = (flt % 3600)//60\n",
        "  s = flt % 60\n",
        "  out = []\n",
        "  if h > 0:\n",
        "    out.append(str(int(h)))\n",
        "    if h == 1:\n",
        "      out.append('hr,')\n",
        "    else:\n",
        "      out.append('hrs,')\n",
        "  if m > 0:\n",
        "    out.append(str(int(m)))\n",
        "    if m == 1:\n",
        "      out.append('min, and')\n",
        "    else:\n",
        "      out.append('mins, and')\n",
        "  out.append(f'{s:.2f}')\n",
        "  out.append('secs')\n",
        "  return ' '.join(out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XPoK-tAHJqaR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 4. Run Training Loop"
      ]
    },
    {
      "metadata": {
        "id": "kRB1G3CCG6Lt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Set hyperparameters\n",
        "MAX_LEN = 32\n",
        "BATCH_SIZE = 128\n",
        "EPOCHS = 2\n",
        "x_dir = data_dir + 'train_input.csv'\n",
        "y_dir = data_dir + 'train_output.csv'\n",
        "NORM_LIM = 3 # value for clip_norm\n",
        "\n",
        "# Load data\n",
        "train_buckets, test_buckets, tkzr, seqs = load_dataset(x_dir, y_dir, MAX_LEN, BATCH_SIZE, drop = True)\n",
        "\n",
        "# Get vocab size\n",
        "num_chars = len(tkzr.word_index) + 1 # Add one for padding\n",
        "embedding_dim = 15\n",
        "enc_hidden = 30\n",
        "dec_hidden = 30\n",
        "\n",
        "# Define model(s)\n",
        "encoder = Encoder(num_chars, embedding_dim, enc_hidden, batch_sz = BATCH_SIZE)\n",
        "decoder = Decoder(num_chars, embedding_dim, dec_hidden, batch_sz = BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "E1QIHlHOHOL5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "checkpoint_dir = data_dir + 'checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WgYulu90Igu7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2207
        },
        "outputId": "e34b75ec-8941-4f0a-ffd5-c31e31b55699"
      },
      "cell_type": "code",
      "source": [
        "# Loop over epochs\n",
        "for epoch in range(EPOCHS):\n",
        "  print(f'Starting Epoch {epoch + 1}\\n')\n",
        "  \n",
        "  start = time.time()\n",
        "  \n",
        "  total_loss = 0\n",
        "  total_batches = 0\n",
        "  val_loss = 0\n",
        "  val_batches = 0\n",
        "\n",
        "  # Loop over buckets\n",
        "  for bucket, dataset in enumerate(train_buckets):\n",
        "    # Reset hidden state\n",
        "    enc_hidden = encoder.initialize_hidden_state()\n",
        "\n",
        "\n",
        "    for inp, targ in dataset.take(-1):\n",
        "      batch_loss = train_step(inp, targ, enc_hidden, NORM_LIM)\n",
        "      \n",
        "      if batch_loss.numpy() == np.nan:\n",
        "        print(f'NaN loss! E = {epoch+1}, B = {bucket + 1}, b = {total_batches}')\n",
        "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "        last_batch = (inp, targ)\n",
        "        \n",
        "      total_loss += batch_loss\n",
        "      \n",
        "      total_batches += 1\n",
        "    \n",
        "      if total_batches % 500 == 0:\n",
        "          print(f'Epoch {epoch + 1} Bucket {bucket + 1} Loss {batch_loss.numpy():.4f} after {total_batches} batches')\n",
        "  \n",
        "  # saving (checkpoint) the model every 2 epochs\n",
        "  if (epoch + 1) % 2 == 0:\n",
        "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "    \n",
        "  # Calculate validation loss\n",
        "  for dataset in test_buckets:\n",
        "    # Reset hidden state\n",
        "    enc_hidden = encoder.initialize_hidden_state()\n",
        "    \n",
        "    for inp, targ in dataset.take(-1):\n",
        "      batch_val_loss = val_step(inp, targ, enc_hidden)\n",
        "      \n",
        "      val_loss += batch_val_loss\n",
        "      val_batches += 1\n",
        "\n",
        "  print(f'\\nEpoch {epoch + 1} Loss {(total_loss / total_batches):.4f} after {total_batches} batches.')\n",
        "  print(f'Tested on {val_batches * BATCH_SIZE} validation examples. val_loss = {val_loss / val_batches}')\n",
        "  print(f'Time taken for 1 epoch: {format_time(time.time() - start)}\\n===========================\\n\\n')"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting Epoch 1\n",
            "\n",
            "Epoch 1 Bucket 21 Loss 1.0739 after 500 batches\n",
            "Epoch 1 Bucket 28 Loss 0.8486 after 1000 batches\n",
            "Epoch 1 Bucket 32 Loss 0.9278 after 1500 batches\n",
            "Epoch 1 Bucket 35 Loss 1.1902 after 2000 batches\n",
            "Epoch 1 Bucket 37 Loss 1.5162 after 2500 batches\n",
            "\n",
            "Epoch 1 Loss 1.1684 after 2645 batches.\n",
            "Tested on 62336 validation examples. val_loss = 1.058976173400879\n",
            "Time taken for 1 epoch: 9 mins, and 57.31 seconds.\n",
            "===========================\n",
            "\n",
            "\n",
            "Starting Epoch 2\n",
            "\n",
            "Epoch 2 Bucket 21 Loss 1.0332 after 500 batches\n",
            "Epoch 2 Bucket 28 Loss 0.8051 after 1000 batches\n",
            "Epoch 2 Bucket 32 Loss 0.8842 after 1500 batches\n",
            "Epoch 2 Bucket 35 Loss 1.1504 after 2000 batches\n",
            "Epoch 2 Bucket 37 Loss 1.4829 after 2500 batches\n",
            "\n",
            "Epoch 2 Loss 1.1292 after 2645 batches.\n",
            "Tested on 62336 validation examples. val_loss = 1.0281800031661987\n",
            "Time taken for 1 epoch: 10 mins, and 5.55 seconds.\n",
            "===========================\n",
            "\n",
            "\n",
            "Starting Epoch 3\n",
            "\n",
            "Epoch 3 Bucket 21 Loss 0.9999 after 500 batches\n",
            "Epoch 3 Bucket 28 Loss 0.7834 after 1000 batches\n",
            "Epoch 3 Bucket 32 Loss 0.8613 after 1500 batches\n",
            "Epoch 3 Bucket 35 Loss 1.1288 after 2000 batches\n",
            "Epoch 3 Bucket 37 Loss 1.4623 after 2500 batches\n",
            "\n",
            "Epoch 3 Loss 1.1057 after 2645 batches.\n",
            "Tested on 62336 validation examples. val_loss = 1.007551908493042\n",
            "Time taken for 1 epoch: 10 mins, and 11.62 seconds.\n",
            "===========================\n",
            "\n",
            "\n",
            "Starting Epoch 4\n",
            "\n",
            "Epoch 4 Bucket 21 Loss 0.9786 after 500 batches\n",
            "Epoch 4 Bucket 28 Loss 0.7688 after 1000 batches\n",
            "Epoch 4 Bucket 32 Loss 0.8440 after 1500 batches\n",
            "Epoch 4 Bucket 35 Loss 1.1078 after 2000 batches\n",
            "Epoch 4 Bucket 37 Loss 1.4395 after 2500 batches\n",
            "\n",
            "Epoch 4 Loss 1.0859 after 2645 batches.\n",
            "Tested on 62336 validation examples. val_loss = 0.9899650812149048\n",
            "Time taken for 1 epoch: 10 mins, and 26.18 seconds.\n",
            "===========================\n",
            "\n",
            "\n",
            "Starting Epoch 5\n",
            "\n",
            "Epoch 5 Bucket 21 Loss 0.9656 after 500 batches\n",
            "Epoch 5 Bucket 28 Loss 0.7560 after 1000 batches\n",
            "Epoch 5 Bucket 32 Loss 0.8310 after 1500 batches\n",
            "Epoch 5 Bucket 35 Loss 1.0917 after 2000 batches\n",
            "Epoch 5 Bucket 37 Loss 1.4256 after 2500 batches\n",
            "\n",
            "Epoch 5 Loss 1.0704 after 2645 batches.\n",
            "Tested on 62336 validation examples. val_loss = 0.9803120493888855\n",
            "Time taken for 1 epoch: 10 mins, and 38.02 seconds.\n",
            "===========================\n",
            "\n",
            "\n",
            "Starting Epoch 6\n",
            "\n",
            "Epoch 6 Bucket 21 Loss 0.9529 after 500 batches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-9d73fc94092a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m       \u001b[0mbatch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNORM_LIM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mbatch_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    412\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1286\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1288\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1290\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m    572\u001b[0m     \"\"\"\n\u001b[1;32m    573\u001b[0m     return self._call_flat(\n\u001b[0;32m--> 574\u001b[0;31m         (t for t in nest.flatten((args, kwargs))\n\u001b[0m\u001b[1;32m    575\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m    576\u001b[0m                            resource_variable_ops.ResourceVariable))))\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    625\u001b[0m     \u001b[0;31m# Only need to override the gradient in graph mode and when we have outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    628\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_register_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args)\u001b[0m\n\u001b[1;32m    413\u001b[0m             attrs=(\"executor_type\", executor_type,\n\u001b[1;32m    414\u001b[0m                    \"config_proto\", config),\n\u001b[0;32m--> 415\u001b[0;31m             ctx=ctx)\n\u001b[0m\u001b[1;32m    416\u001b[0m       \u001b[0;31m# Replace empty list with None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     59\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "z2xFuk5wRuo-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# See How it Goes"
      ]
    },
    {
      "metadata": {
        "id": "XPqhXHE7Ry_k",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def clean_ocr(article, tkzr, start_char = 'स', end_char = 'ए'):\n",
        "\n",
        "    # Tokenise the article\n",
        "    inputs = [inp_lang.word_index[i] for i in list(article)]\n",
        "    \n",
        "    # Set threshold to stop outputting results if 'end' character not reached\n",
        "    give_up = int(len(inputs) * 1.2)\n",
        "    \n",
        "    # Convert to tensor\n",
        "    inputs = tf.convert_to_tensor(inputs)\n",
        "    \n",
        "    # Create empty string for result\n",
        "    result = ''\n",
        "    \n",
        "    # Encode the article\n",
        "    hidden = np.zeroes(1, encoder.enc_units)\n",
        "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "\n",
        "    # Ready the decoder\n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = tf.expand_dims([tkzr.word_index[start_char]], 0)\n",
        "\n",
        "    # Start outputting characters till the end character is reached, or\n",
        "    # the output sequence is getting much longer than the input\n",
        "    while len(result) < give_up:\n",
        "      # Get next result from decoder:\n",
        "      predictions, dec_hidden, _ = decoder(dec_input,\n",
        "                                           dec_hidden,\n",
        "                                           enc_out)\n",
        "      \n",
        "      # Which is the next letter?\n",
        "      predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "      \n",
        "      # Add to result\n",
        "      result += tkzr.index_word[predicted_id]\n",
        "\n",
        "      # If we've reached the end, stop\n",
        "      if tkzr.index_word[predicted_id] == end_char:\n",
        "          break\n",
        "\n",
        "      # Otherwise the predicted ID is fed back into the model\n",
        "      dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    return result, article"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}