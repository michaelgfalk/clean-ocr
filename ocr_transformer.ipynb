{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ocr_transformer.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/michaelgfalk/clean-ocr/blob/master/ocr_transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "XQdbN2CEIgt5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Waves of Words: Correcting Trove's Messy OCR\n",
        "\n",
        "One aim of the *Waves of Words* project is to extract Aboriginal wordlists from [Trove](https://trove.nla.gov.au). A challenge we face is that historical newspapers are difficult to OCR, so many of the texts are riddled with errors.\n",
        "\n",
        "Using the training data available from the ALTA 2017 OCR competition, can we create a model that will clean the text enough for our aboriginal word detector to work?\n",
        "\n",
        "I have been giving some thought to whether uppercase letters and punctuation should be preserved in this model, given that the aim is to clean up the text for our detector, which only requires lower case letters and ignores punctuation. I think we need to include all the characters in this one. The extra information about sentence barriers, for example, should hopefully help the model as it would a human when it tries to correct the text. Moreover, many OCR errors involve exchaning punctuation or digits for letters, e.g. `l = 1 = !`.\n",
        "\n",
        "**References:**\n",
        "\n",
        "* D. Mollá, S. Cassidy. Overview of the 2017 ALTA Shared Task:\n",
        "Correcting OCR Errors (2017). *Proc. ALTA 2017*.\n",
        "[https://aclanthology.coli.uni-saarland.de/papers/U17-1014/u17-1014](https://aclanthology.coli.uni-saarland.de/papers/U17-1014/u17-1014)"
      ]
    },
    {
      "metadata": {
        "id": "PzfYL5jcnT7m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Install TensorFlow2\n",
        "!pip install -q tensorflow-gpu==2.0.0-alpha0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ePS0gU47IW-u",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "from itertools import product"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-V1HqBKxgqry",
        "colab_type": "code",
        "outputId": "e6267e81-b17a-4d3e-ef5f-a7dfaacdf090",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "# Mount google drive to get training data. Set data_dir\n",
        "drive.mount('/content/gdrive')\n",
        "data_dir = '/content/gdrive/My Drive/waves_of_words/ocr_correction_data/'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vsXRVxW2SdT_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Import test and training data\n",
        "raw_x = pd.read_csv(data_dir + 'train_input.csv')\n",
        "raw_y = pd.read_csv(data_dir + 'train_output.csv')\n",
        "test_x = pd.read_csv(data_dir + 'test_input.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fC1sXVhtmlBa",
        "colab_type": "code",
        "outputId": "388ab02a-0e88-4d78-ec15-ed2ab693ce60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "cell_type": "code",
      "source": [
        "# What is the shape of the corpus?\n",
        "print(\"Summary of raw_x:\\n\")\n",
        "display(raw_x['original'].str.len().describe())\n",
        "print(\"\\n\\nSummary of raw_y:\\n\")\n",
        "display(raw_y['solution'].str.len().describe())\n",
        "print(f\"\\n\\nThere are {raw_x['original'].str.len().sum()} characters in the training set.\")\n",
        "print(f\"\\n\\n90% of the training examples have {raw_x['original'].str.len().quantile(0.9)} characters or less\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Summary of raw_x:\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "count     6000.000000\n",
              "mean      2738.592000\n",
              "std       3687.978419\n",
              "min        107.000000\n",
              "25%        675.000000\n",
              "50%       1416.000000\n",
              "75%       3369.000000\n",
              "max      48424.000000\n",
              "Name: original, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Summary of raw_y:\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "count     6000.000000\n",
              "mean      2737.897500\n",
              "std       3695.634174\n",
              "min         75.000000\n",
              "25%        671.000000\n",
              "50%       1413.000000\n",
              "75%       3374.250000\n",
              "max      48500.000000\n",
              "Name: solution, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "There are 16431552 characters in the training set.\n",
            "\n",
            "\n",
            "90% of the training examples have 6464.0 characters or less\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4Q3AVjwcpENo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def chunk_text(article_string, chunk_size = 200, start_char = \"<START>\", end_char = \"<END>\"):\n",
        "  \"\"\"Chunk Trove articles from dataset.\n",
        "  \n",
        "  Arguments:\n",
        "  ==========\n",
        "  article_string (str): the entire article as a single string\n",
        "  chunk_size (int): the length of the desired chunks, in characters\n",
        "  start (str): the token for the beginning of an article\n",
        "  end (str): the token for the end of an article\n",
        "  \n",
        "  Returns:\n",
        "  ==========\n",
        "  chunks (list): a list of chunks\"\"\"\n",
        "  \n",
        "  # Ensure 'start' and 'end' are not present in the string\n",
        "  if start in article_string or end in article_string:\n",
        "    raise Exception(\"Start or end token found in string\")\n",
        "  \n",
        "  # If not, add placeholders for special characters...\n",
        "  article_string = \"S\" + article_string + \"E\"\n",
        "  # ... and chunk\n",
        "  chunks = []\n",
        "  num_chars = len(article_string)\n",
        "  for i in range(0, num_chars, chunk_size):\n",
        "    sub_strt = i\n",
        "    sub_end = min(num_chars, i + chunk_size)\n",
        "    chunks.append(article_string[sub_strt:sub_end])\n",
        "  \n",
        "  # Replace special characters\n",
        "  chunks[0] = re.sub(\"^S\", start_char, chunks[0])\n",
        "  chunks[-1] = re.sub(\"E$\", end_char, chunks[-1])\n",
        "  \n",
        "  return chunks"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dxjNyGtL5rSo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Since the sequences don't line up, we will need to use a 'stateful' RNN to connect all the chunks during training...\n",
        "\n",
        "To do this, we need to split the training data into two levels of batches. There will be $k$ hyperbatches, each containing $l$ training examples. "
      ]
    },
    {
      "metadata": {
        "id": "BJTDEdDkhtNp",
        "colab_type": "code",
        "outputId": "87fc536a-de39-4fa3-e047-e07176516731",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "cell_type": "code",
      "source": [
        "num_articles = len(raw_x)\n",
        "chunk_size = 200\n",
        "q = 0.85\n",
        "ninety_percentile = max(raw_x['original'].str.len().quantile(q) + 2, raw_y['solution'].str.len().quantile(q) + 2)\n",
        "max_chunks = np.ceil(ninety_percentile / chunk_size)\n",
        "batch_size = 256\n",
        "num_hyper_batches = np.ceil(num_articles/batch_size)\n",
        "\n",
        "\n",
        "print(f'There are {num_articles} articles in the training data.')\n",
        "print(f'Let us split each article into chunks of {chunk_size} characters,')\n",
        "print(f'and cap the number of chunks at the {int(q * 100)}th percentile.')\n",
        "print(f'{q * 100}% of articles have {ninety_percentile:.2f} characters or less (including start and end tokens).')\n",
        "print(f'This equates to {max_chunks} chunks per article.')\n",
        "print(f'If we choose a batch_size of {batch_size}, there will be {num_hyper_batches} hyper-batches,')\n",
        "print(f'comprising {batch_size * max_chunks} training examples each.')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 6000 articles in the training data.\n",
            "Let us split each article into chunks of 200 characters,\n",
            "and cap the number of chunks at the 85th percentile.\n",
            "85.0% of articles have 5108.15 characters or less (including start and end tokens).\n",
            "This equates to 26.0 chunks per article.\n",
            "If we choose a batch_size of 256, there will be 24.0 hyper-batches,\n",
            "comprising 6656.0 training examples each.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "WJxwYd0_j3L4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "What would be most efficient, actually, is to dynamically create the hyper-batches by sorting the training examples in order of length. Then each hyper-batch could have its own `max_chunks` hyperparameter. Meanwhile $t$ and the `batch_size` would stay the same for each hyper-batch."
      ]
    },
    {
      "metadata": {
        "id": "YFcjbkxkARMW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Set hyperparameters\n",
        "chunk_size = 200\n",
        "start_char = \"स\" # 's' in devanagari\n",
        "end_char = \"ए\" # 'e' in devanagari\n",
        "batch_size = 256\n",
        "num_articles = len(raw_x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZzKy_74HrqE1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Join DataFrames\n",
        "train_joined = pd.merge(raw_x, raw_y, on = 'id')\n",
        "\n",
        "# Sort in order of string length\n",
        "train_joined['max_len'] = pd.concat(\n",
        "    [train_joined['original'].str.len(), train_joined['solution'].str.len()],\n",
        "    axis = 1\n",
        ").max(axis = 1)\n",
        "train_joined = train_joined.sort_values(by = 'max_len')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ItVz8LTzrHaq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Fit a tokenizer to the data\n",
        "tkzr = Tokenizer(\n",
        "    num_words = None,\n",
        "    filters = None,\n",
        "    lower = False,\n",
        "    char_level = True\n",
        ")\n",
        "\n",
        "# Show it the X data\n",
        "tkzr.fit_on_texts(train_joined['original'])\n",
        "# Show it the Y data\n",
        "tkzr.fit_on_texts(train_joined['solution'])\n",
        "# Show it the special start and end characters\n",
        "tkzr.fit_on_texts([start_char,end_char])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yBAMqzqt05PW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Add start and end, tokenize, pad, chunk\n",
        "hyper_batches = []\n",
        "\n",
        "# Start and end tokens\n",
        "train_joined['original'] = start_char + train_joined['original'] + end_char\n",
        "train_joined['solution'] = start_char + train_joined['solution'] + end_char\n",
        "\n",
        "# Tokenise\n",
        "x_tokens = tkzr.texts_to_sequences(train_joined['original'])\n",
        "y_tokens = tkzr.texts_to_sequences(train_joined['solution'])\n",
        "\n",
        "# Iterate over hyper_batches:\n",
        "for i in range(0, num_articles, batch_size):\n",
        "  hyper_batch = {}\n",
        "  \n",
        "  # Determine slice start and end points\n",
        "  end = min(i + batch_size, num_articles)\n",
        "  \n",
        "  # Get articles for this batch\n",
        "  batch_x = x_tokens[i:end]\n",
        "  batch_y = y_tokens[i:end]\n",
        "  \n",
        "  # Determine max_len\n",
        "  max_len = max([len(x) for x in batch_x] + [len(y) for y in batch_y])\n",
        "  # Round up to chunk_size\n",
        "  max_len += chunk_size - (max_len % chunk_size)\n",
        "  \n",
        "  # Pad sequences\n",
        "  x_padded = pad_sequences(batch_x, maxlen = max_len)\n",
        "  y_padded = pad_sequences(batch_y, maxlen = max_len)\n",
        "  \n",
        "  # Split and stack\n",
        "  num_chunks = int(max_len / chunk_size)\n",
        "  hyper_batch['X'] = np.concatenate(np.split(x_padded, num_chunks, axis = 1), axis = 0)\n",
        "  hyper_batch['Y'] = np.concatenate(np.split(y_padded, num_chunks, axis = 1), axis = 0)\n",
        "  \n",
        "  # Create index\n",
        "  hyper_batch['index'] = [x for x in product(range(0,num_chunks), range(0,batch_size))]\n",
        "  \n",
        "  # Append\n",
        "  hyper_batches.append(hyper_batch)\n",
        "  \n",
        "  # Sanity check\n",
        "  print(f'\\nNext hyperbatch:')\n",
        "  print(f'max_len = {max_len}')\n",
        "  print(f'chunk_size = {chunk_size}')\n",
        "  print(f'num_chunks = {num_chunks}')\n",
        "  print(f'shape of X: {hyper_batch[\"X\"].shape}')\n",
        "  print(f'shape of Y: {hyper_batch[\"Y\"].shape}')\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FGbyx2pHK1jR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This works, but the last three hyperbatches are gonna have a lot of zeros in them..."
      ]
    }
  ]
}