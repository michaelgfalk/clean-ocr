{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ocr_transformer.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/michaelgfalk/clean-ocr/blob/master/ocr_transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "XQdbN2CEIgt5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Waves of Words: Correcting Trove's Messy OCR\n",
        "\n",
        "One aim of the *Waves of Words* project is to extract Aboriginal wordlists from [Trove](https://trove.nla.gov.au). A challenge we face is that historical newspapers are difficult to OCR, so many of the texts are riddled with errors.\n",
        "\n",
        "Using the training data available from the ALTA 2017 OCR competition, can we create a model that will clean the text enough for our aboriginal word detector to work?\n",
        "\n",
        "I have been giving some thought to whether uppercase letters and punctuation should be preserved in this model, given that the aim is to clean up the text for our detector, which only requires lower case letters and ignores punctuation. I think we need to include all the characters in this one. The extra information about sentence barriers, for example, should hopefully help the model as it would a human when it tries to correct the text. Moreover, many OCR errors involve exchaning punctuation or digits for letters, e.g. `l = 1 = !`.\n",
        "\n",
        "**References:**\n",
        "\n",
        "* D. Mollá, S. Cassidy. Overview of the 2017 ALTA Shared Task:\n",
        "Correcting OCR Errors (2017). *Proc. ALTA 2017*.\n",
        "[https://aclanthology.coli.uni-saarland.de/papers/U17-1014/u17-1014](https://aclanthology.coli.uni-saarland.de/papers/U17-1014/u17-1014)"
      ]
    },
    {
      "metadata": {
        "id": "PzfYL5jcnT7m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Install TensorFlow2\n",
        "!pip install -q tensorflow-gpu==2.0.0-alpha0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ePS0gU47IW-u",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "from collections import defaultdict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-V1HqBKxgqry",
        "colab_type": "code",
        "outputId": "e6267e81-b17a-4d3e-ef5f-a7dfaacdf090",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "# Mount google drive to get training data. Set data_dir\n",
        "drive.mount('/content/gdrive')\n",
        "data_dir = '/content/gdrive/My Drive/waves_of_words/ocr_correction_data/'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vsXRVxW2SdT_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Import test and training data\n",
        "raw_x = pd.read_csv(data_dir + 'train_input.csv')\n",
        "raw_y = pd.read_csv(data_dir + 'train_output.csv')\n",
        "test_x = pd.read_csv(data_dir + 'test_input.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fC1sXVhtmlBa",
        "colab_type": "code",
        "outputId": "388ab02a-0e88-4d78-ec15-ed2ab693ce60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "cell_type": "code",
      "source": [
        "# What is the shape of the corpus?\n",
        "print(\"Summary of raw_x:\\n\")\n",
        "display(raw_x['original'].str.len().describe())\n",
        "print(\"\\n\\nSummary of raw_y:\\n\")\n",
        "display(raw_y['solution'].str.len().describe())\n",
        "print(f\"\\n\\nThere are {raw_x['original'].str.len().sum()} characters in the training set.\")\n",
        "print(f\"\\n\\n90% of the training examples have {raw_x['original'].str.len().quantile(0.9)} characters or less\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Summary of raw_x:\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "count     6000.000000\n",
              "mean      2738.592000\n",
              "std       3687.978419\n",
              "min        107.000000\n",
              "25%        675.000000\n",
              "50%       1416.000000\n",
              "75%       3369.000000\n",
              "max      48424.000000\n",
              "Name: original, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Summary of raw_y:\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "count     6000.000000\n",
              "mean      2737.897500\n",
              "std       3695.634174\n",
              "min         75.000000\n",
              "25%        671.000000\n",
              "50%       1413.000000\n",
              "75%       3374.250000\n",
              "max      48500.000000\n",
              "Name: solution, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "There are 16431552 characters in the training set.\n",
            "\n",
            "\n",
            "90% of the training examples have 6464.0 characters or less\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4Q3AVjwcpENo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def chunk_text(article_string, chunk_size = 200, start_char = \"<START>\", end_char = \"<END>\"):\n",
        "  \"\"\"Chunk Trove articles from dataset.\n",
        "  \n",
        "  Arguments:\n",
        "  ==========\n",
        "  article_string (str): the entire article as a single string\n",
        "  chunk_size (int): the length of the desired chunks, in characters\n",
        "  start (str): the token for the beginning of an article\n",
        "  end (str): the token for the end of an article\n",
        "  \n",
        "  Returns:\n",
        "  ==========\n",
        "  chunks (list): a list of chunks\"\"\"\n",
        "  \n",
        "  # Ensure 'start' and 'end' are not present in the string\n",
        "  if start in article_string or end in article_string:\n",
        "    raise Exception(\"Start or end token found in string\")\n",
        "  \n",
        "  # If not, add placeholders for special characters...\n",
        "  article_string = \"S\" + article_string + \"E\"\n",
        "  # ... and chunk\n",
        "  chunks = []\n",
        "  num_chars = len(article_string)\n",
        "  for i in range(0, num_chars, chunk_size):\n",
        "    sub_strt = i\n",
        "    sub_end = min(num_chars, i + chunk_size)\n",
        "    chunks.append(article_string[sub_strt:sub_end])\n",
        "  \n",
        "  # Replace special characters\n",
        "  chunks[0] = re.sub(\"^S\", start_char, chunks[0])\n",
        "  chunks[-1] = re.sub(\"E$\", end_char, chunks[-1])\n",
        "  \n",
        "  return chunks"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dxjNyGtL5rSo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Since the sequences don't line up, we will need to use a 'stateful' RNN to connect all the chunks during training...\n",
        "\n",
        "To do this, we need to split the training data into two levels of batches. There will be $k$ hyperbatches, each containing $l$ training examples. "
      ]
    },
    {
      "metadata": {
        "id": "BJTDEdDkhtNp",
        "colab_type": "code",
        "outputId": "87fc536a-de39-4fa3-e047-e07176516731",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "cell_type": "code",
      "source": [
        "num_articles = len(raw_x)\n",
        "chunk_size = 200\n",
        "q = 0.85\n",
        "ninety_percentile = max(raw_x['original'].str.len().quantile(q) + 2, raw_y['solution'].str.len().quantile(q) + 2)\n",
        "max_chunks = np.ceil(ninety_percentile / chunk_size)\n",
        "batch_size = 256\n",
        "num_hyper_batches = np.ceil(num_articles/batch_size)\n",
        "\n",
        "\n",
        "print(f'There are {num_articles} articles in the training data.')\n",
        "print(f'Let us split each article into chunks of {chunk_size} characters,')\n",
        "print(f'and cap the number of chunks at the {int(q * 100)}th percentile.')\n",
        "print(f'{q * 100}% of articles have {ninety_percentile:.2f} characters or less (including start and end tokens).')\n",
        "print(f'This equates to {max_chunks} chunks per article.')\n",
        "print(f'If we choose a batch_size of {batch_size}, there will be {num_hyper_batches} hyper-batches,')\n",
        "print(f'comprising {batch_size * max_chunks} training examples each.')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 6000 articles in the training data.\n",
            "Let us split each article into chunks of 200 characters,\n",
            "and cap the number of chunks at the 85th percentile.\n",
            "85.0% of articles have 5108.15 characters or less (including start and end tokens).\n",
            "This equates to 26.0 chunks per article.\n",
            "If we choose a batch_size of 256, there will be 24.0 hyper-batches,\n",
            "comprising 6656.0 training examples each.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "WJxwYd0_j3L4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "What would be most efficient, actually, is to dynamically create the hyper-batches by sorting the training examples in order of length. Then each hyper-batch could have its own `max_chunks` hyperparameter. Meanwhile $t$ and the `batch_size` would stay the same for each hyper-batch."
      ]
    },
    {
      "metadata": {
        "id": "ZzKy_74HrqE1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Apply chunking function to training data:\n",
        "\n",
        "# Create empty dict to receive training chunks\n",
        "chunked_training_pairs = defaultdict(dict)\n",
        "\n",
        "# Set hyperparameters\n",
        "chunk_size = 200\n",
        "start_char = \"स\" # 's' in devanagari\n",
        "end_char = \"ए\" # 'e' in devanagari\n",
        "\n",
        "# Join DataFrames\n",
        "train_joined = pd.merge(raw_x, raw_y, on = 'id')\n",
        "\n",
        "# Sort in order of string length\n",
        "train_joined['max_len'] = pd.concat(\n",
        "    [train_joined['original'].str.len(), train_joined['solution'].str.len()],\n",
        "    axis = 1\n",
        ").max(axis = 1)\n",
        "train_joined = train_joined.sort_values(by = 'max_len')\n",
        "\n",
        "# Iterate over joint frame\n",
        "for tup in train_joined.itertuples(False, None):\n",
        "  article_id, x_string, y_string, _ = tup\n",
        "  x_chunks = chunk_text(x_string, chunk_size)\n",
        "  y_chunks = chunk_text(y_string, chunk_size)\n",
        "  for chunk_id, chunk in enumerate(x_chunks):\n",
        "    chunked_training_pairs[(article_id, chunk_id)]['x'] = chunk\n",
        "  for chunk_id, chunk in enumerate(y_chunks):\n",
        "    chunked_training_pairs[(article_id, chunk_id)]['y'] = chunk"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ItVz8LTzrHaq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Fit a tokenizer to the data\n",
        "tkzr = tf.keras.preprocessing.text.Tokenizer(\n",
        "    num_words = None,\n",
        "    filters = None,\n",
        "    lower = False,\n",
        "    char_level = True\n",
        ")\n",
        "tkzr.fit_on_texts(raw_x['original'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yBAMqzqt05PW",
        "colab_type": "code",
        "outputId": "d4d8d96b-5d7f-4f4a-bc2d-99838a835d55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        }
      },
      "cell_type": "code",
      "source": [
        "# Create list of training data hyperbatches:\n",
        "batch_size = 256\n",
        "num_articles = len(train_joined)\n",
        "hyper_batches = []\n",
        "for i in range(0, len(train_joined), batch_size):\n",
        "  # Initialise dict\n",
        "  hyper_batch = {}\n",
        "  # Get indices for next slice\n",
        "  s, e = i, min(i + batch_size, num_articles)\n",
        "  # Iterate over slice and chunk\n",
        "  for tup in train_joined.iloc[s:e].itertuples(False, None):\n",
        "    article_id, x_string, y_string, _ = tup\n",
        "    x_chunks = chunk_text(x_string, chunk_size)\n",
        "    y_chunks = chunk_text(y_string, chunk_size)\n",
        "    for chunk_id, chunk in enumerate(x_chunks):\n",
        "      hyper_batch[(article_id, chunk_id)]['x'] = chunk\n",
        "    for chunk_id, chunk in enumerate(y_chunks):\n",
        "      hyper_batch[(article_id, chunk_id)]['y'] = chunk\n",
        "  \n",
        "  \n",
        "  \n",
        "  hyper_batches.append(hyper_batch)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-95-ce8af4b2e973>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m{\u001b[0m\u001b[0;34m'the quick brown fox'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "AqF4NtK72nqX",
        "colab_type": "code",
        "outputId": "a15fc999-afe8-40a5-dca2-c945824b8816",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "cell_type": "code",
      "source": [
        "chunked_training_pairs[(1000,0)]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'x': '<START>Death Of Veteran Trainer The death occurred yes-| terday of Mr. Thomas David j Kilmartin, who had been as-j sociated with racing forj many years. He was 75. One of his best horses was the; hurdler Co',\n",
              " 'y': '<START>Death Of Veteran Trainer The death occurred yesterday of Mr. Thomas David Kilmartin, who had been associated with racing for many years. He was 75. One of his best horses was the hurdler Common King.'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        }
      ]
    },
    {
      "metadata": {
        "id": "wEXp0ngo4Gig",
        "colab_type": "code",
        "outputId": "9b244e26-5329-4d84-d520-76862a7bb5e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "tkzr.texts_to_sequences(['hello'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[10, 2, 11, 11, 7]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 104
        }
      ]
    }
  ]
}