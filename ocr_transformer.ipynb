{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ocr_transformer.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/michaelgfalk/clean-ocr/blob/master/ocr_transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "XQdbN2CEIgt5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Waves of Words: Correcting Trove's Messy OCR\n",
        "\n",
        "One aim of the *Waves of Words* project is to extract Aboriginal wordlists from [Trove](https://trove.nla.gov.au). A challenge we face is that historical newspapers are difficult to OCR, so many of the texts are riddled with errors.\n",
        "\n",
        "Using the training data available from the ALTA 2017 OCR competition, can we create a model that will clean the text enough for our aboriginal word detector to work?\n",
        "\n",
        "I have been giving some thought to whether uppercase letters and punctuation should be preserved in this model, given that the aim is to clean up the text for our detector, which only requires lower case letters and ignores punctuation. I think we need to include all the characters in this one. The extra information about sentence barriers, for example, should hopefully help the model as it would a human when it tries to correct the text. Moreover, many OCR errors involve exchaning punctuation or digits for letters, e.g. `l = 1 = !`.\n",
        "\n",
        "**References:**\n",
        "\n",
        "* D. Mollá, S. Cassidy. Overview of the 2017 ALTA Shared Task:\n",
        "Correcting OCR Errors (2017). *Proc. ALTA 2017*.\n",
        "[https://aclanthology.coli.uni-saarland.de/papers/U17-1014/u17-1014](https://aclanthology.coli.uni-saarland.de/papers/U17-1014/u17-1014)"
      ]
    },
    {
      "metadata": {
        "id": "PzfYL5jcnT7m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Install TensorFlow2\n",
        "!pip install -q tensorflow-gpu==2.0.0-alpha0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ePS0gU47IW-u",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "import csv\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-V1HqBKxgqry",
        "colab_type": "code",
        "outputId": "4c78c963-7254-41ea-8acc-d0745bd4012e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "# Mount google drive to get training data. Set data_dir\n",
        "drive.mount('/content/gdrive')\n",
        "data_dir = '/content/gdrive/My Drive/waves_of_words/ocr_correction_data/'"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Yur9PBcBzjBp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 1. Data Pipeline (new)\n",
        "\n",
        "This new data pipeline makes use of Tensorflows Dataset object to manage the extraction and transformation more efficiently."
      ]
    },
    {
      "metadata": {
        "id": "-JwKCURv0I44",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_dataset(source, target, start = 'स', end = 'ए'):\n",
        "  \"\"\"Imports articles from csv and packages for training.\"\"\"\n",
        "  \n",
        "  # Lists for import\n",
        "  raw_x = []\n",
        "  raw_y = []\n",
        "  \n",
        "  # Import raw text\n",
        "  with open(source, \"rt\") as f:\n",
        "    x_reader = csv.reader(f, delimiter = ',', quotechar = '\"')\n",
        "    for row in x_reader:\n",
        "      raw_x.append(row[1])\n",
        "  with open(target, \"rt\") as f:\n",
        "    y_reader = csv.reader(f, delimiter = ',', quotechar = '\"')\n",
        "    for row in y_reader:\n",
        "      raw_y.append(row[1])\n",
        "  \n",
        "  # Drop header rows\n",
        "  raw_x = raw_x[1:]\n",
        "  raw_y = raw_y[1:]\n",
        "  \n",
        "  # Add special characters\n",
        "  x = [start + article + end for article in raw_x if start not in article and end not in article]\n",
        "  y = [start + article + end for article in raw_y if start not in article and end not in article]\n",
        "  \n",
        "  return x, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_KIK6ErA3rf5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def tokenize(source, target):\n",
        "  \"\"\"Instantiates tokenizer, fits and applies.\"\"\"\n",
        "  \n",
        "  # Fit tokenizer\n",
        "  tkzr = tf.keras.preprocessing.text.Tokenizer(\n",
        "    num_words = None,\n",
        "    filters = None,\n",
        "    lower = False,\n",
        "    char_level = True\n",
        "  )\n",
        "  tkzr.fit_on_texts(source + target)\n",
        "  \n",
        "  # Apply to texts\n",
        "  x = tkzr.texts_to_sequences(source)\n",
        "  y = tkzr.texts_to_sequences(target)\n",
        "  \n",
        "  return x, y, tkzr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vO17zE6HPV-n",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def split_and_stack(x, y, max_len, batch_size, drop = False):\n",
        "  \"\"\"Takes as input two python lists, and outputs a list of tensor buckets.\n",
        "  \n",
        "  Arguments:\n",
        "  ==========\n",
        "  x (list): the tokenized source strings\n",
        "  y (list): the tokenized target strings\n",
        "  max_len (int): the maximum number of time steps the model will consider\n",
        "  batch_size (int): the batch size for the training examples\n",
        "  drop (bool): keep the final batch, if len(x) is not a multiple of batch_size?\n",
        "  \n",
        "  Returns:\n",
        "  ==========\n",
        "  bucket_list (list): a list of length m, each item of which is a bucket of\n",
        "    similar-length numpy array\n",
        "  \n",
        "  A bucket is a tensor of shape (2, m_prime, max_len). Training examples are first\n",
        "  bucketed into groups of similar length, and seperated into batches of batch_size.\n",
        "  Each batch is then padded out to an integer multiple of max_len, split into chunks\n",
        "  of length max_len and stacked.\n",
        "  \n",
        "  This bucketing, splitting and stacking allows the data to be fed to a stateful RNN.\n",
        "  \n",
        "  Dimensions:\n",
        "  -The first dimension seperates x examples from y\n",
        "  -The second dimension seperates individual training examples.\n",
        "    m_prime = batch_size * ⌈max_len_x_or_y_within_batch / batch_size⌉ \n",
        "  -The third dimension seperates individual time-steps, and is fixed at max_len\"\"\"\n",
        "  \n",
        "  # Set number of training examples (round up if not dropping)\n",
        "  assert len(x) == len(y)\n",
        "  m = len(x)\n",
        "  \n",
        "  # Sort x and y by sequence length\n",
        "  x_y = sorted(zip(x,y), key = lambda tup: max(len(tup[0]), len(tup[1])), reverse = True)\n",
        "  \n",
        "  # Loop through list and create batches\n",
        "  bucket_list = []\n",
        "  for i in range(0, m, batch_size):\n",
        "    \n",
        "    # Slice and unpack the list\n",
        "    bn = batch_size\n",
        "    if len(x_y) < batch_size and drop:\n",
        "      break\n",
        "    elif len(x_y) < batch_size and not drop:\n",
        "      bn = len(x_y)\n",
        "    \n",
        "    bx, by = zip(*[x_y.pop() for _ in range(bn)])\n",
        "    \n",
        "    # Calculate length boundary and m_prime\n",
        "    bl = max(len(bx[0]), len(by[0]))\n",
        "    b = max_len - (bl % max_len) + bl\n",
        "    m_prime = int(b / max_len)\n",
        "    \n",
        "    # Pad the sequences\n",
        "    x_pad = pad_sequences(bx, maxlen = b)\n",
        "    y_pad = pad_sequences(by, maxlen = b)\n",
        "    \n",
        "    # Split and stack\n",
        "    x_out = np.concatenate(np.split(x_pad, m_prime, axis = 1), axis = 0)\n",
        "    y_out = np.concatenate(np.split(y_pad, m_prime, axis = 1), axis = 0)\n",
        "    \n",
        "    # Covert to dataset and append to list\n",
        "    bucket = tf.data.Dataset.from_tensor_slices((x_out, y_out))\n",
        "    bucket = bucket.batch(bn) # NB: bn = batch_size except on the last batch, if drop != True\n",
        "    bucket_list.append(bucket)\n",
        "    \n",
        "  return bucket_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0xWguIxG5VHm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_dataset(x_path, y_path, max_len = 200, batch_size = 64, drop = False, test_size = .2):\n",
        "  \n",
        "  x, y = create_dataset(x_path, y_path)\n",
        "  \n",
        "  x, y, tkzr = tokenize(x, y)\n",
        "  \n",
        "  x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = test_size)\n",
        "  \n",
        "  bucket_list = split_and_stack(x, y, max_len, batch_size, drop)\n",
        "  \n",
        "  return x_train, x_test, y_train, y_test, bucket_list, tkzr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rd5Q8T5S6Xl1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def convert(tkzr, tensor, n = 100):\n",
        "  text = [tkzr.index_word[x] for x in tensor[:n] if x != 0]\n",
        "  text = ''.join(text + ['...'])\n",
        "  print(text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IsoTxaMM6Rr4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 2. Model Definition\n",
        "\n",
        "Adapted from TensorFlow docs."
      ]
    },
    {
      "metadata": {
        "id": "lOKqMawV6VJB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, num_chars, embedding_dim, enc_units, batch_sz):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.enc_units = enc_units\n",
        "    self.embedding = tf.keras.layers.Embedding(num_chars, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.enc_units, \n",
        "                                   # The following parameters must be set this way\n",
        "                                   # to use CuDNN on GPU\n",
        "                                   activation='tanh',\n",
        "                                   recurrent_activation='sigmoid',\n",
        "                                   recurrent_dropout=0,\n",
        "                                   unroll=False,\n",
        "                                   use_bias=True,\n",
        "                                   reset_after=True,\n",
        "                                   # The following parameters are necessary for the\n",
        "                                   # encoder-decoder architecture\n",
        "                                   return_sequences=True, \n",
        "                                   return_state=True,\n",
        "                                   # Stateful must be 'True' in order\n",
        "                                   # to link the batches in each hyperbatch\n",
        "                                   stateful=True,\n",
        "                                   # Just the standard initializer\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "\n",
        "  def call(self, x, hidden):\n",
        "    x = self.embedding(x)\n",
        "    output, state = self.gru(x, initial_state = hidden)        \n",
        "    return output, state\n",
        "\n",
        "  def initialize_hidden_state(self):\n",
        "    return tf.zeros((self.batch_sz, self.enc_units))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FDgYbbYn7GiP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class BahdanauAttention(tf.keras.Model):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "  \n",
        "  def call(self, query, values):\n",
        "    # hidden shape == (batch_size, hidden size)\n",
        "    # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "    # we are doing this to perform addition to calculate the score\n",
        "    hidden_with_time_axis = tf.expand_dims(query, 1)\n",
        "\n",
        "    # score shape == (batch_size, max_length, hidden_size)\n",
        "    score = self.V(tf.nn.tanh(\n",
        "        self.W1(values) + self.W2(hidden_with_time_axis)))\n",
        "\n",
        "    # attention_weights shape == (batch_size, max_length, 1)\n",
        "    # we get 1 at the last axis because we are applying score to self.V\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "    # context_vector shape after sum == (batch_size, hidden_size)\n",
        "    context_vector = attention_weights * values\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "    \n",
        "    return context_vector, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yPbOQ2H27a4b",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, num_chars, embedding_dim, dec_units, batch_sz):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.dec_units = dec_units\n",
        "    self.embedding = tf.keras.layers.Embedding(num_chars, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.dec_units, \n",
        "                                   # The following parameters must be set this way\n",
        "                                   # to use CuDNN on GPU\n",
        "                                   activation='tanh',\n",
        "                                   recurrent_activation='sigmoid',\n",
        "                                   recurrent_dropout=0,\n",
        "                                   unroll=False,\n",
        "                                   use_bias=True,\n",
        "                                   reset_after=True,\n",
        "                                   # The following parameters are necessary for the\n",
        "                                   # encoder-decoder architecture\n",
        "                                   return_sequences=True, \n",
        "                                   return_state=True,\n",
        "                                   # Stateful must be 'True' in order\n",
        "                                   # to link the batches in each hyperbatch\n",
        "                                   stateful=True,\n",
        "                                   # Just the standard initializer\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    self.fc = tf.keras.layers.Dense(num_chars)\n",
        "\n",
        "    # used for attention\n",
        "    self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "  def call(self, x, hidden, enc_output):\n",
        "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "\n",
        "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "    # passing the concatenated vector to the GRU\n",
        "    output, state = self.gru(x)\n",
        "\n",
        "    # output shape == (batch_size * 1, hidden_size)\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "    # output shape == (batch_size, vocab)\n",
        "    x = self.fc(output)\n",
        "\n",
        "    return x, state, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-ow7AR1fHKMp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 3. Set up Training"
      ]
    },
    {
      "metadata": {
        "id": "7ta0lQ46HNHb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "  \n",
        "  return tf.reduce_mean(loss_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IfTMbgXjHRwP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "  loss = 0\n",
        "        \n",
        "  with tf.GradientTape() as tape:\n",
        "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "\n",
        "    dec_input = tf.expand_dims(inp[:,0], 1)\n",
        "\n",
        "    # Teacher forcing - feeding the target as the next input\n",
        "    for t in range(1, targ.shape[1]):\n",
        "      # passing enc_output to the decoder\n",
        "      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "\n",
        "      loss += loss_function(targ[:, t], predictions)\n",
        "\n",
        "      # using teacher forcing\n",
        "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "  batch_loss = (loss / int(targ.shape[1]))\n",
        "\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "  \n",
        "  return batch_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XPoK-tAHJqaR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 4. Run Training Loop"
      ]
    },
    {
      "metadata": {
        "id": "kRB1G3CCG6Lt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Set hyperparameters\n",
        "MAX_LEN = 200\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 10\n",
        "x_dir = data_dir + 'train_input.csv'\n",
        "y_dir = data_dir + 'train_output.csv'\n",
        "\n",
        "# Load data\n",
        "x_train, x_test, y_train, y_test, bucket_list, tkzr = load_dataset(x_dir, y_dir, MAX_LEN, BATCH_SIZE)\n",
        "\n",
        "# Get vocab size\n",
        "num_chars = len(tkzr.word_index)\n",
        "embedding_dim = 15\n",
        "\n",
        "# Define model(s)\n",
        "encoder = Encoder(num_chars, embedding_dim, 30, batch_sz = BATCH_SIZE)\n",
        "decoder = Decoder(num_chars, embedding_dim, 30, batch_sz = BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "E1QIHlHOHOL5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "checkpoint_dir = data_dir + 'checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WgYulu90Igu7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Loop over epochs\n",
        "for epoch in range(1):\n",
        "  start = time.time()\n",
        "  \n",
        "  total_loss = 0\n",
        "  total_batches = 0\n",
        "\n",
        "  # Loop over buckets\n",
        "  for bucket, dataset in enumerate(bucket_list):\n",
        "    # Reset hidden state\n",
        "    enc_hidden = encoder.initialize_hidden_state()\n",
        "\n",
        "\n",
        "    for inp, targ in dataset.take(-1):\n",
        "      batch_loss = train_step(inp, targ, enc_hidden)\n",
        "      total_loss += batch_loss\n",
        "      \n",
        "      total_batches += 1\n",
        "    \n",
        "      if total_batches % 3 == 0:\n",
        "          print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                       batch,\n",
        "                                                       batch_loss.numpy()))\n",
        "  # saving (checkpoint) the model every 2 epochs\n",
        "  if (epoch + 1) % 2 == 0:\n",
        "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                      total_loss / steps_per_epoch))\n",
        "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FDYxXqFPB493",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "At the moment, this doesn't seem to work..."
      ]
    }
  ]
}